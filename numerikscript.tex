\documentclass[12pt]{article}


\usepackage{algorithmic} %Für Pseudocode https://math-linux.com/latex-26/faq/latex-faq/article/how-to-write-algorithm-and-pseudocode-in-latex-usepackage-algorithm-usepackage-algorithmic
\usepackage{stmaryrd} %Für Widerspruchsblitz
\usepackage{amsmath} 
\usepackage{amssymb}
\usepackage{amsthm} %Für Theoreme und Beweise
\usepackage{graphicx} %Für Bilder
\usepackage{nicefrac}

\newtheoremstyle{break}% name
  {}%         Space above, empty = `usual value'
  {}%         Space below
  {\normalfont}% Body font
  {}%         Indent amount (empty = no indent, \parindent = para indent)
  {\bfseries}% Thm head font
  {.}%        Punctuation after thm head
  {\newline}% Space after thm head: \newline = linebreak
  {}%         Thm head spec

\theoremstyle{break}

\renewcommand{\thesection}{\Roman{section}}
\counterwithout{subsection}{section}
%\renewcommand{\thesubsection}{\arabic{subsection}}

%Definiere Satz, Definition,...
\newtheorem{theorem}{Satz}[subsection]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{korollar}[theorem]{Korollar}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{algorithm}[theorem]{Algorithmus}
\newtheorem{comment}[theorem]{Bemerkung}
\newtheorem*{comment*}{Bemerkung}
\newtheorem*{example*}{Beispiel}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{example}[theorem]{Beispiel}
\newtheorem{nothing}[theorem]{}

\author{Prof. Schaedle}
\title{Numerik 1}

\begin{document}
\maketitle

\newpage

\section{Numerische Integration}

\input{Kapitel_1/Abschnitt_1} %Einführung
\input{Kapitel_1/Abschnitt_2} %Ordnung von Quadraturformeln
\input{Kapitel_1/Abschnitt_3} %Quadraturfehler
\input{Kapitel_1/Abschnitt_4} %Quadratur mit hoher Ordnung
\input{Kapitel_1/Abschnitt_5} %Orthogonalpolynome
\input{Kapitel_1/Abschnitt_6} %Ein adaptives Programm
\input{Kapitel_1/Abschnitt_7} %Gauß- und Lobatto Quadraturformeln

\section{Interpolation und Approximation}

\begin{description}
  \item[Problemstellung A]
    Zu gegebenen $(x_0, y_0), ...,(x_n, y_n)$ berechne Polynom $p$ vom Grad $\leq n$ mit $$p(x_j) = y_j, \quad j=0,...,n$$
  
  \item[Problemstellung B]
    $f:[a,b] \rightarrow \mathbb{R}$ gegeben. Finde einfach auszuwertende Funktion $p: [a,b] \rightarrow \mathbb{R}$, etwa ein Polynom, stückweises Polynom, rationale Funktion, sodass $f-p$ klein ist.
    \begin{enumerate}
      \item[i)] $f(x)=p(x)$ für endlich viele vorgegebene Punkte $x$
      \item[ii)] $\int_a^b (f(x)-p(x))^2 dx$ soll minimal sein.
      \item[iii)] $\max_{x \in [a,b]} \vert f(x) -p(x) \vert$ soll minimal sein.
    \end{enumerate}
\end{description}

\subsection{Newtonsche Interpolationsformel}

\begin{example}
\begin{description}\item \end{description}
\begin{description}
  \item n=1: \\
    $(x_0, y_0),(x_1,y_1)$, $p \in \mathcal{P}_1$ das beide Punkte verbindet.\\
    $$p(x) = y_0 + (x-x_0) \frac{y_1-y_0}{x_1-x_0}$$
  \item n=2: \\
    $(x_0, y_0),(x_1,y_1),(x_2,y_2)$ \\
    $$p(x) = y_0 + (x-x_0) \frac{y_1-y_0}{x_1-x_0} + a(x-x_0)(x-x_1)$$
    Bestimme $a$ so, dass $p(x_2) = y_2$
    \begin{flalign*}
    y_2 &\overset{!}{=} y_0 + (\overset{-x_1+x_1}{\check{x_2\thinspace-}}x_0) \frac{y_1-y_0}{x_1-x_0} + a(x_2-x_0)(x-x_1)&\\
    a(x_2-x_0)(x_2-x_1) &= y_2 - y_0 - (x_2-x_1) \frac{y_1-y_0}{x_1-x_0} - y_1 + y_0 &\\
    \Rightarrow a &= \frac{1}{x_2-x_0} \left( \frac{y_2-y_1}{x_2-x_1} - \frac{y_1-y_0}{x_1-x_0} \right) 
     \end{flalign*}
\end{description}
\end{example}

\begin{definition}[dividierte Differenzen]
Für $(x_0,y_0), (x_1, y_1), ..., (x_n, y_n)$ mit paarweise verschiedenen Stützstellen $x_j$ definieren wir
\begin{flalign*}
y[x_j] &:= y_j \quad \left( = \delta^0 y[x_j] \right) &\\
\delta y[x_j, x_{j+1}] &:= \frac{y_{j+1} - y_j}{x_{j+1}-x_j} = \frac{\delta^0 y[x_{j+1}]-\delta^0 y[x_{j}]}{x_{j+1} - x_j} &\\
\delta ^2 y[x_j, x_{j+1}, x_{j+2}] &:= \frac{\delta y[x_{j+1}, x_{j+2}]-\delta y[x_{j}, x_{j+1}]}{x_{j+2} - x_j} &\\
\delta ^k y[x_j, x_{j+1},..., x_{j+k}] &:= \frac{1}{x_{j+k}-x_j} \left( \delta^{k-1} y[x_{j+1}, ..., x_{j+k}] - \delta^{k-1} y[x_j, ..., x_{j+k-1}] \right)
\end{flalign*}
\underline{Schema:}\\
\begin{tabular}{ccccc}
 
$x_0$ & $y_0$& & &\\
 & & $\delta^1y[x_0, x_1]$ & &\\
$x_1$ & $y_1$ & &$\delta^2y[x_0, x_1, x_2]$ &\\
 & & $\delta^1y[x_1, x_2]$ & & $\delta^3y[x_0, x_1, x_2, x_3]$\\
$x_2$ & $y_2$ & & $\delta^2y[x_1, x_2, x_3]$ &\\
 & & $\delta^1y[x_2, x_3]$ & &\\
$x_3$ & $y_3$ & & &\\
 
\end{tabular}
\end{definition}

\begin{comment}
Falls die $x_i$ äquidistant, dh. $x_i = x_0+ih$ so ist: \\
\begin{flalign*}
\delta y[x_i, x_{i+1}] &= \frac{y_{i+1} - y_i}{h} =: \frac{1}{h} \Delta y_i &\\
\delta ^2 y[x_i, x_{i+1}, x_{i+2}] &= \frac{\frac{1}{h} \Delta y_{i+1} - \frac{1}{h} \Delta y_{i}}{2h} = \frac{1}{2h^2} \Delta ^2 y_i &\\
\delta ^k y[x_i, ..., x_{i+k}] &= \frac{1}{k!h^k} \Delta^k y_i,
\end{flalign*}
wobei $\Delta^k := \Delta^{k-1}y_{i+1} - \Delta^{k-1}y_i$.
\end{comment}

\begin{theorem}[Newtonsche Interpolationsformel]
Zu paarweise verschiedenen reellen $x_i$, $i=0,..., n$, existiert ein eindeutiges Polynom $p \in \mathcal{P}_n$ durch die Punkte $(x_i, y_i)$, $i=0,...,n$ (d.h. $p(x_i) = y_i$ für $i=1,...,n$). Es lässt sich berechnen durch:
\begin{flalign*}
p(x) &= y[x_0] + (x-x_0) \delta y[x_0,x_1] + ... + (x-x_0)(x-x_1)...(x-x_{n-1}) \delta ^n y[x_0, ..., x_n] &\\
&= \sum_{i=0}^n \prod_{j=0}^{i-1} (x-x_j) \delta^iy[x_0,...,x_i] 
\end{flalign*}
\begin{proof}[Beweis](Induktion)\phantom{\qedhere}
\begin{description}
  \item[IA] $n=1$ (und $n=2$) vgl. Beispiel (1.1)
  \item[IS] $n-1 \rightarrow n$\\
    $$p_0(x) = y[x_0] + (x-x_0) \delta y[x_1, x_0] + ... + (x-x_0)...(x-x_{n-2}) \delta ^{n-1}y[x_0,..., x_{n-1}]$$ 
    ist das eindeutige interpolierende Polynom mit 
    $$\text{deg}(p_0) \leq n-1$$
    zu $(x_0,y_0), (x_1, y_1), ..., (x_{n-1}, y_{n-1})$. \\
    Für den Ansatz
    $$p(x) = p_0(x) + a(x-x_0)(x-x_1)...(x-x_{n-1})$$
    ergibt die Forderung $p(x_n) = y_n$
    $$a = \frac{y_n-p_0(x_n)}{(x_n-x_0)(x_n-x_1)...(x_n-x_{n-1})}$$
    Da $a$ eindeutig ist, ist $p$ eindeutig.\\
    Es bleibt zu zeigen: $a = \delta^n y[x_0, ..., x_n]$\\
    Sei dazu ein Polynom $p_1(x)$, welches durch $(x_1, y_1), ..., (x_n, y_n)$ läuft, mit $\text{deg}(p_1) \leq n-1$. Nach Induktionsannahme gilt 
    \begin{flalign*}
    p_1(x) &= y[x_1] + (x-x_1) \delta^1y[x_1, x_2] + ... + (x-x_1)...(x-x_{n-1}) \delta ^{n-1}y[x_1, ..., x_n]&\\
    &=x^{n-1} \delta^{n-1}y[x_1, ..., x_n] + r
    \end{flalign*}
    mit $\text{deg}(r) \leq n-2$.\\
    Setze Polynom
    $$p(x) := \frac{x_n-x}{x_n-x_0} p_0(x) + \frac{x-x_0}{x_n-x_0}p_1(x)$$
    mit $\text{deg}(p) \leq n$ durch $(x_0, y_0), ..., (x_n, y_n)$. \\
    Das gilt, da: 
    \begin{flalign*}
    p(x_0) &= p_0(x_0) = y_0 &\\
    p(x_n) &= p_1(x_n) = y_n &\\
    \text{Für } i=1,...,n-1: &\\
    p(x_i) &= \frac{x_n-x_i}{x_n-x_0} \underbrace{p_0(x_i)}_{y_i} + \frac{x_i-x_0}{x_n-x_0} \underbrace{p_1(x_i)}_{y_i} = y_i
    \end{flalign*}
    Andererseits:
    $$p(x) = ax^n + r \quad \text{mit  deg}(r) \leq n-1$$
    Koeffizientenvergleich:
    \begin{align*}
    a &= - \frac{1}{x_n-x_0} \delta^{n-1}y[x_0, ..., x_{n-1}] + \frac{1}{x_n-x_0} \delta^{n-1}y[x_1, ..., x_{n}] &\\
    &= \delta^n y[x_0, ..., x_n]&\\\tag*{\qed}
    \end{align*}
    
\end{description}
\end{proof}
\end{theorem}

\begin{nothing}[Hornerschema]
Zur Auswertung des Interpolationspolynom $p$ an der Stelle $x$ verwendet man 
$$
p(x) = y[x_0] + (x-x_0) \left( \delta y[x_0, x_1] + (x-x_1) \left( \delta ^2 y[x_0, x_1, x_2] + (x-x_2) \left(  ... \left( \delta^n y[x_0, ..., x_n] \right) \right) \right) \right)
$$
\underline{Algorithmus:}
\begin{algorithmic}
\STATE $s = \delta^n y[x_0, ..., x_n]$
\FOR{$k = n-1, ...,0$}
\STATE $s = \delta^k y[x_0, ..., x_k] + (x-x_k) s$
\ENDFOR
\end{algorithmic}
\end{nothing}

\begin{example}
\begin{tabular}{||l|l|l|rrrr||}
\hline
$i$ & $x_i$ & $y_i$ & $\delta^1 y[x_0, x_1]$& $\delta^2 y[x_0, x_1, x_2]$& $\delta^3 y[x_0, .., x_3]$& $\delta^4 y[x_0, .., x_4]$\\
\hline
$0$ & $-1$ & $0$&&&&\\
& & & $\frac{1-0}{0-(-1)} = 1$&&&\\
$1$ & $0$ & $1$ & & $\frac{0-1}{2-(-1)} = -\frac{1}{3}$&&\\
& & & $0$ & & $\frac{\frac{2}{3} - (-\frac{1}{3})}{3-(-1)} = \frac{1}{4}$&\\
$2$ & $2$ & $1$ & & $\frac{2-0}{3-0} = \frac{2}{3}$ && $\frac{-\frac{2}{5} - \frac{1}{4}}{5-(-1)} = -\frac{13}{120}$\\
& & & $\frac{3-1}{3-2} = 2$ & & $\frac{-\frac{4}{3} - \frac{2}{3}}{5-0} = -\frac{2}{5}$&\\
$3$ & $3$ & $3$ & & $\frac{-2-2}{5-2} = -\frac{4}{3}$&&\\
& & & $\frac{-1-3}{5-3} = -2$&&&\\
$4$ & $5$ & $-1$&&&&\\
\hline
\end{tabular} \\\\
Das Interpolationspolynom ist also
$$p(x) = 0 + (x+1) *1 - \frac{1}{3}(x+1)(x) + \frac{1}{4}(x+1)x(x-2)+(x+1)x(x-2)(x-3)\left(-\frac{13}{120}\right)$$
bzw. nach Hornerschema
\begin{flalign*}
p(x) &= 0+(x+1)\left(1+ x\left(-\frac{1}{3} + (x-2)\left(\frac{1}{4} + (x-3)\left(-\frac{13}{120}\right)\right)\right)\right) &\\
\end{flalign*}
Werte $p(x)$ an der Stelle 1 aus:
\begin{flalign*}
-\frac{13}{120}* (-2) &= \frac{26}{120} &\\
\left( \frac{1}{4} + \frac{26}{120} \right)(-1) &= -\frac{56}{120} = -\frac{7}{15} &\\
\left( -\frac{7}{15} - \frac{1}{3}\right) 1 &= -\frac{12}{15} = -\frac{4}{5} &\\
\left(-\frac{4}{5} + 1\right)2 &= \frac{2}{5} = p(1) &\\
\end{flalign*}
\end{example}

\subsection{Fehler bei der Polynominterpolation}

\underline{Problem:} $f: \thinspace [a,b] \rightarrow \mathbb{R}$ werde interpoliert in Stützstellen $x_0, ..., x_n \in [a,b]$ durch $p \in \mathcal{P}_n$ mit $p(x_i) = f(x_i)$ für $i=0,...,n$. \\
Wie groß ist der Fehler $f(x)-p(x)$?

\begin{theorem}
Sei $f: [a,b] \rightarrow \mathbb{R}$ (n+1)-mal stetig differenzierbar, $p \in \mathcal{P}_n$ mit $p(x_i) = f(x_i)$ ($i=0,...,n$) das Interpolationspolynom zu paarweise verschiedenen Stützstellen $x_i \in [a,b]$ ($i=0,..., n$). Dann gilt: \\
\[\forall x \in [a,b] \exists \xi = \xi(x) \in (a,b): f(x)-p(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{i=0}^n (x-x_i) \]
\begin{proof}[Beweis]
Siehe (9.4)
\end{proof}
\end{theorem}

\begin{example}[Berechnung von Logarithmentafeln: Briggs, 17. Jhd]
$f(x) = log_{10}(x), \quad x \in [55, 58]$\\
Wähle Stützstellen:\\
$x_0 = 55, \quad x_1 = 56, \quad x_2 = 57, \quad x_3=58$ \\
Es seien \[log_{10}(55), \medspace log_{10}(56), \medspace log_{10}(57) \text{ und } log_{10}(58)\] bereits bekannt. Berechne eine Näherung von \(f\) an bei \(log_{10}(56.5)\) \\
$\rightarrow$ Interpolationspolynom $p$:
\begin{flalign*}
log_{10}(65.5) &= 1.752048448 &\\
p(56.5) &= 1.75204845 &\\
f'(x) &= \frac{1}{ln(10)x} &\\
f''(x) &= -\frac{1}{ln(10)x^2} &\\
f^{(3)}(x) &= \frac{2}{ln(10)x^3} &\\
f^{(4)}(x) &= -\frac{6}{ln(10)x^4} &\\
\text{Für } x\in[55, 58]: &\\
\vert f^{(4)}(x)\vert &\leq \frac{6}{55^4 ln(10)}
\Rightarrow &\\
\vert log_{10}(56.5) - p(56.5) \vert &\leq 1.5 * 0.5 * 0.5 * 1.5 * \frac{6}{55^4ln(10) \frac{1}{4!}} &\\
&\approx 6.7 * 10^{-9}
\end{flalign*}
\end{example}

Für den Beweis von (9.1) wird folgendes Lemma benötigt:

\begin{lemma}
Sei $f \in C^n([a,b])$ und sei für paarweise verschiedene $x_i \in [a,b]$ ($i=0,...,n$) $y_i := f(x_i)$. Dann existiert $\xi \in (\min_i(x_i), \max_i(x_i))$, sodass 
$$\delta^ny[x_0,..., x_n] = \frac{f^{(n)}(\xi)}{n!}\quad (x_0 < x_1 < ... < x_n)$$
\begin{proof}[Beweis]
Sei $p$ ein Interpolationspolynom zu $(x_i, y_i)_{i=0}^n$. Setzt man $d:= p-f$, so gilt $d(x_i) = 0$ für $i=0,...,n$.\\
n-maliges anwenden des Mittelwertsatzes liefert paarweise verschiedene $\xi_i$, $(i=0, ...,n-1)$ mit $d'(\xi_i) = 0$ für $\xi_i \in (\min_j(x_j), \max_j(x_j))$. \\
Dasselbe Argument angewandt auf $d'$ liefert $\eta_0,..., \eta_{n-2}$ mit $d''(\eta_i) = 0$ für $i=0,..., n-2$.\\
Wiederhole dies bis:\\
Es existiert $\rho_0$ mit $d^{(n)}(\rho_0) = 0$\\
$\Rightarrow f^{(n)}(\rho_0) = p^{(n)}(\rho_0) = n! \delta^ny[x_0,..., x_n],$\\
da $\delta^ny[x_0, ..., x_n]$ der Koeffizient von $x^n$ in $p$ ist.
\end{proof}
\end{lemma}

\begin{comment*}
Für $n=1$ ist Lemma (9.3) der Mittelwertsatz (oder Satz von Rolle) aus Ana I:
$$ \exists \xi: \frac{f(x_1)-f(x_2)}{x_1-x_2} = f'(\xi)$$
\end{comment*}

\begin{nothing}[Beweis von (9.1)]
Sei $\bar{x} \in [a,b]$ beliebig.
\begin{description}
  \item[1. Fall] $\bar{x} = x_i$ für ein $i \in \{0,...,n\}$, so ist wegen $p(x_i) - f(x_i) = 0$ nichts zu zeigen.
  
  \item[2. Fall] $\bar{x} \neq x_i$ für alle $i \in \{0,...,n\}$. Sei $\bar{p}$ das Interpolationspolynom mit $\text{deg}(\bar{p}) \leq n+1$ zu $(x_i, f(x_i))_{i=0}^n$ und $(\bar{x}, f(\bar{x}))$. Die Newton'sche Interpolationsformel liefert dann
  \begin{align*}
  \bar{p}(x) &= p(x) + \prod_{i=0}^n(x-x_i)\delta^{n+1}y[x_0,...,x_n, \bar{x}] &\\
  &\underset{(9.3)}{=} p(x) + \prod_{i=0}^n(x-x_i) \frac{f^{(n+1)}(\xi)}{(n+1)!}
  \end{align*}
  Für $x = \bar{x}$ gilt $\bar{p}(\bar{x}) = f(\bar{x})$. Damit ist Satz (9.1) für $x\in[a,b]$ gezeigt.\qed
\end{description}
\end{nothing}

\underline{Fragen:}
\begin{itemize}
  \item Für welche Wahl der Stützstellen $x_i$ ($i=0,...,n$, n fest) ist 
    $$\max_{x\in[a,b]} \left\vert \prod_{i=0}^n(x-x_i)\right\vert$$
    minimal? (Siehe Abschnitt 10)
  \item Wie wirken sich Fehler in den Funktionsauswertungen (etwa Messfehler oder Rechenfehler) auf das Interpolationspolynom aus?
\end{itemize}

\begin{theorem}[Lagrange Interpolationsformel]
Das Interpolationspolynom $p$ zu $(x_i, y_i)_{i=0}^n$ ist gegeben durch 
$$p(x) = \sum_{i=0}^n y_il_i(x)$$
mit 
$$l_i(x) = \frac{\prod_{j=0,\thinspace j\neq i}^n (x-x_j) }{\prod_{j=0,\thinspace j\neq i}^n ( x_i-x_j)}$$
\begin{proof}[Beweis]
$\text{deg}(l_i) = n$, $l_i(x_j) = \left\{
\begin{array}{ll}
1 & \textrm{für } j=i \\
0 & \, \textrm{sonst} \\
\end{array}
\right.$ \\
$\Rightarrow p(x_j) = \sum_{i=0}^n y_il_i(x_j) = y_j$
\end{proof}
\end{theorem}

\begin{comment*}
Lagranges und Newtons Interpolationsformeln liefern beide das gleiche Polynom nur in unterschiedlichen Darstellungen.
\end{comment*}

\begin{definition}
$$\Lambda_n := \max_{x\in[a,b]} \sum_{i=0}^n\vert l_i(x) \vert$$
heißt die \textbf{Lebesgue Konstante} zu den Stützstellen $x_i$, $i=0,...,n$ auf dem Intervall $[a,b]$.
\end{definition}

Damit gilt:

\begin{theorem}
Sei $p$ das Interpolationspolynom (vom Grad $\leq n$) zu $(x_i, y_i)_{i=0}^n$ und $\tilde{p}$ das Interpolationspolynom zu $(x_i, \tilde{y}_i)_{i=0}^n$, so gilt:
$$ \max_{x \in [a,b]} \vert p(x) - \tilde{p}(x) \vert \leq \Lambda_n \max_{i=0,...,n}\vert y_i - \tilde{y}_i \vert $$
\begin{proof}[Beweis]
klar
\end{proof}
\end{theorem}

\begin{example}
\begin{description}\item \end{description}
\begin{itemize}
  \item Für äquidistante Stützstellen $x_i = a + i\frac{b-a}{n}$ ($i=0,...,n$) ist 
    \begin{align*}
    \Lambda_{10} &\approx 40 &\\
    \Lambda_{20} &\approx 3*10^4 &\\
    \Lambda_{40} &\approx 10^{10} &\\
    \Lambda_{n} &\approx \frac{2^n}{ln(n)*e*n} \quad \text{für } n \rightarrow \infty&\\
    \end{align*}
    $\Rightarrow$ Vorsicht bei Polynominterpolation mit vielen äquidistanten Stützstellen! \\
    In $\S$10 werden wir Stützstellen kennenlernen mit $\Lambda_n \leq 4$ für $n \leq 100$.
\end{itemize}
\end{example}

\begin{theorem}
Sei $f: [a,b] \rightarrow \mathbb{R}$ stetig, $p$ Interpolationspolynom zu $f$ in den Stützstellen $x_0,..., x_n \in [a,b]$. So gilt:
\[ \forall q \in \mathcal{P}_{n+1}: \quad \max_{x \in [a,b] } \vert f(x) -p(x) \vert \leq (1+ \Lambda_n) \max_{x \in [a,b]} \vert q(x) - f(x) \vert .\]
Hierbei ist $\Lambda_n$ die Lebesgue-Konstante zu $(x_i)_{i=0}^n$ auf $[a,b]$.
\begin{proof}[Beweis]
Sei $q \in \mathcal{P}$.
\[ f-p = (f-q) + (q-p)\]
$q$ ist das Interpolationspolynom zu sich selbst in den $x_0,..., x_n$. Nach (9.7) gilt für $y_i = f(x_i)$ $\tilde{y_i}  = q(x_i)$.
\begin{align*}
\max_{x \in [a,b]} \vert p(x) - q(x) \vert &\leq \Lambda_n \max_{i=0,...,n} \vert f(x_i) - q(x_i) \vert &\\
&\leq \Lambda_n \max_{x \in [a,b]} \vert f(x)-q(x) \vert &\\
\Rightarrow \max_{x \in [a,b]} \vert f(x) - p(x) \vert &\leq \max_{x \in [a,b]} \vert f(x)-q(x) \vert + \max_{x \in [a,b]} \vert p(x)-q(x) \vert &\\
&\leq (1+ \Lambda_n) \max_{x \in [a,b]} \vert q(x) - f(x) \vert 
\end{align*}
\end{proof}
\end{theorem}

\subsection{Tschebyscheff-Interpolation}

\underline{Ziel:} Interpoliere $f: [a,b] \rightarrow \mathbb{R}$ in "guten" Stützstellen. \\
Ohne Einschränkungen sei $[a,b] = [-1, 1]$

\begin{definition}
$T_n(x) = \cos(n * \arccos(x))$ für $x \in [-1, 1]$ heißt n-tes Tschebyscheff-Polynom.
\end{definition}

\begin{lemma}
$T_n(x)$ ist für $x \in [-1, 1]$ ein Polynom mit folgenden Eigenschaften:
\begin{enumerate}
  \item[i)] $T_0(x) = 1$, $T_1(x) = x$
  \item[ii)] $T_n(x) = 2^{n-1} x^n + r(x)$ mit $r_n \in \mathcal{P}_{n-1}$
  \item[iii)] $T_{n+1}(x) = 2xT_n(x) - T_{n-1}(x)$
  \item[iv)] $\forall x \in [-1, 1]: \quad \vert T_n(x) \vert \leq 1$
  \item[v)] $T_n(cos(\frac{k\pi}{n})) = (-1)^k, \quad k=0,...,n$
  \item[vi)] $T_n(cos(\frac{(2k+1)\pi}{2n})) = 0, \quad k=0,...,n-1$ 
\end{enumerate}
\begin{proof}[Beweis]\leavevmode
\begin{enumerate}
  \item[i)] klar, da $T_0(x) = cos(0) = 1$, $T_1(x) = cos(arccos(x)) = x, \quad x \in [a,b]$
  \item[iii)]$cos((n+1)\phi) + cos((n-1)\phi)$\\
    $= \medspace cos(n\phi)cos(\phi) - sin(n\phi)sin(\phi) + cos(n\phi)cos(-\phi) - sin(n\phi)sin(-\phi)$ \\
    $= \medspace 2cos(n\phi)cos(\phi)$
  \item[ii)] folgt aus i) und iii)
  \item[iv)] klar, da $cos: [-1, 1] \rightarrow \mathbb{R}$
  \item[v) + vi)] ebenfalls klar, da $T_n(cos(\frac{k\pi}{n})) = cos(n \frac{k\pi}{n}) = cos(k\pi) = (-1)^k$\\
  analog: $T_n(cos(\frac{(2k+1)\pi}{2n})) = cos(n \frac{(2k+1)\pi}{2n})  = 0$
\end{enumerate}
\end{proof}
\end{lemma}

\begin{example} \leavevmode
\begin{description}
  \item $T_2(x) = 2x^2 - 1$
  \item $T_3(x) = 2x(2x^2 -1) - x = 4x^3 -3x$
\end{description}
\end{example}

\begin{lemma}
Sei $q \in \mathcal{P}_n$, $q(x) = 2^{n-1} x^n + r(x)$ mit $r(x) \in \mathcal{P}_{n-1}, q \neq T_n$. Dann gilt
\[ \max_{x\in [-1, 1]} \vert q(x) \vert > \max_{x\in [-1, 1]} \vert T_n(x) \vert \quad(=1)\]

\begin{proof}[Beweis]\leavevmode
Annahme: $\forall x \in [-1, 1]: \quad \vert q(x) \vert \leq 1$
\begin{description}
  \item $T_n(1) = 1$
  \item $T_n(cos(\frac{\pi}{n})) = -1$
\end{description}
Nach dem Zwischenwertsatz hat $q-T_n$ eine Nullstelle im Intervall $[cos(\frac{\pi}{n}), 1]$. Falls ein "Randpunkt" $x$ eine Nullstelle ist, so handelt es sich um eine doppelte Nullstelle, da $q'(x) = 0 = T_n'(x)$. Ebenso existiert in $[cos(\frac{2\pi}{n}), cos(\frac{\pi}{n})]$ und allgemein in $[cos(\frac{(k+1)\pi}{n}), cos(\frac{k\pi}{n})]$ für $k=0,...,n-1$.\\
Nullstelle $\Rightarrow$ $q-T_n$ hat n Nullstellen.\\
Andererseits ist $q-T_n \in \mathcal{P}_{n-1} \quad \Rightarrow q-T_n \equiv 0 \quad \Rightarrow q = T_n \quad \lightning$
\end{proof}
\end{lemma}

\begin{theorem}
Unter allen Unterteilungen $\{x_0,...,x_n\}$ von $[-1,1]$ wird 
$$ \max_{x \in [-1, 1]} \vert (x-x_0)(x-x_1)...(x-x_n) \vert $$
minimal für $x_k = cos\left( \frac{2k+1}{n+1} \frac{\pi}{2}\right), \quad k=0,...,n$ (d.h. $x_k$ sind die Wurzeln von $T_{n+1}$)

\begin{proof}[Beweis]
Nach Lemma (10.4) wird $\max_{x \in [-1, 1]} \vert (x-x_0)(x-x_1)...(x-x_n) \vert$ minimal gdw.  $(x-x_0)...(x-x_n) = 2^{-n} T_{n+1}(x)$, d.h. falls $x_k$ Wurzeln von $T_{n+1}$ sind.
\end{proof}
\end{theorem}

\begin{theorem}
Die Lebesguekonstanten $\Lambda_n$ zu den Tschebyscheffknoten (Wurzeln von $T_{n+1}$) erfüllen
\begin{description}
  \item $\Lambda_n \leq 3$ für $n\leq 20$
  \item $\Lambda_n \leq 4$ für $n\leq 100$
  \item $\Lambda_n \approx \frac{2}{\pi} log(n)$ für $n \rightarrow \infty$
\end{description}
\begin{proof}[Beweis]
ohne Beweis.
\end{proof}
\end{theorem}
Nach Satz (9.9) liefert die Interpolation in den Wurzeln der Tschebyscheffpolynome eine fast optimale Polynominterpolation an $f$.\\
Dazu kommen Eigenschaften, die die Berechnung eines Interpolationspolynoms in den Tschebyscheffknoten (Wurzeln der Tschebyscheffpolynome) vereinfachen.

\begin{lemma}
Die Tschebyscheffpolynome sind orthogonal, bzgl. des Skalarprodukts 
\[ \langle f, g \rangle := \int_{-1}^1 f(x)g(x) \frac{1}{\sqrt{1-x^2}} dx\]

\begin{proof}[Beweis]
Übungsaufgabe
\end{proof}
\end{lemma}

\begin{lemma}
Die Tschebyscheffpolynome $T_k, k = 0, ..., n$ sind orthogonal bzgl. des Skalarprodukts (auf $\mathcal{P}_n$)
\[(f, g) := \sum_{l=0}^n f(x_l)g(x_l), \quad \text{wobei } x_0, ..., x_n \medspace \text{Wurzeln von } T_{n+1}(x) \]

\begin{proof}[Beweis]\leavevmode
\begin{align*}
T_k(x_l) &= \text{cos}\left(k *\text{arccos}\left( \text{cos} \left( \frac{2l-1}{n+1} \frac{\pi}{2}\right)\right)\right) &\\
&= \text{cos}\left( k \frac{2l+1}{n+1} \frac{\pi}{2}\right) &\\
&= \text{cos}\left(k \left(l+ \frac{1}{2}\right)h \right)
\end{align*}
für $h = \frac{\pi}{n+1}$\\
Damit ist 
\begin{align*}
(T_k, T_j) &= \sum_{l=0}^n \text{cos}\left(k\left(l+\frac{1}{2}\right)h\right) * \text{cos}\left(j\left(l+\frac{1}{2}\right)h\right), &\\
\intertext{da $\text{cos}(x)\text{cos}(y) = \frac{1}{2}(\text{cos}(x+y) + \text{cos}(x-y))$}
&= \frac{1}{2} \sum_{l=0}^n \text{cos}\left(\left(k+j\right)\left(l+\frac{1}{2}\right)h\right) * \text{cos}\left((k-j)\left(l+\frac{1}{2}\right)h\right) &\\
\intertext{Es gilt: $\text{cos}(x) = \text{Re} (e^{ix})$}
&= \frac{1}{2} \text{Re}\left( \sum_{l=0}^n e^{i(k+j)(l+\frac{1}{2})h} + e^{i(k-j)(l+\frac{1}{2})h}\right) &\\
&= \frac{1}{2} \text{Re}\left( \sum_{l=0}^n \left( e^{i(k+j)lh}e^{i(k+j)\frac{h}{2}} + e^{i(k-j)lh}e^{i(k-j)\frac{h}{2}}\right)\right) &\\
&= \frac{1}{2} \text{Re} \left( e^{i(k+j)\frac{h}{2}} \frac{e^{i(k+j)h(n+1)} -1}{ e^{i(k+j)h} - 1} + e^{i(k-j)\frac{h}{2}} \frac{e^{i(k-j)h(n+1)} -1}{ e^{i(k-j)h} - 1}\right), \quad \text{für }k \neq j &\\
\intertext{Es gilt $k(n+1) = \pi$}
&\overset{\text{Behauptung}}{=} \begin{cases}
0 & k \neq j \\
\frac{1}{2} (n+1) & k=j\neq 0 \\
(n+1) & k = j = 0
\end{cases}
\end{align*}
\begin{description}
  \item[Fall 1:] $k=j=0 \Rightarrow \frac{1}{2} \sum_{l=0}^n(1+1) = (n+1)$
  \item[Fall 2:] $k=j\neq0 \Rightarrow \frac{1}{2} \text{Re} \left( (n+1) + e^{ijh} \underbrace{\frac{e^{i2j\overbrace{(n+1)h}^{= \pi}}-1}{e^{i2jh}-1}}_{=0}\right) = \frac{1}{2} (n+1)$ 
  \item[Fall 3:] $k \neq j$:
    \begin{description}
      \item[Fall 1:] $k+j$ ist gerade $\Rightarrow$ $k-j$ ist gerade \\
        $\Rightarrow \medspace \frac{1}{2} \text{Re} \left(0+0\right) = 0$
      \item[Fall 2:] $k+j$ ist ungerade $\Rightarrow$ $k-j$ ist ungerade \\
        \begin{align*}
        &\Rightarrow \medspace  \frac{1}{2} \text{Re} \left( e^{i(k+j)\frac{h}{2}} \frac{-2}{ e^{i(k+j)h} - 1} + e^{i(k-j)\frac{h}{2}} \frac{-2}{ e^{i(k-j)h} - 1}\right)&\\
        &= \frac{1}{2} \text{Re} \left(\underbrace{\frac{-2}{ e^{i(k+j)\frac{h}{2}} + e^{-i(k+j)\frac{h}{2}}}}_{\text{rein imaginär}} +  \underbrace{\frac{-2}{ e^{i(k-j)\frac{h}{2}} - e^{-i(k-j)\frac{h}{2}}}}_{\text{rein imaginär}}\right)&\\
        &= 0
        \end{align*}
    \end{description}
\end{description}
\end{proof}
\end{lemma}

\begin{comment*}
$(\thinspace \cdot \thinspace, \thinspace \cdot \thinspace )$ ist ein Skalarprodukt auf $\mathcal{P}_n$, da 
\begin{enumerate}
  \item[i)] bilinear
  \item[ii)] symmetrisch
  \item[iii)] positiv definit \\
    $(f,f) = \sum_{l=0}^n f(x_l)^2 \geq 0$\\
    $(f,f) = 0 \overset{!}{\Rightarrow} f \equiv 0$\\
    $\sum_{l=0}^n f(x_l)^2 = 0 \Rightarrow \forall l: \medspace f(x_l) = 0$
    $\Leftrightarrow f \equiv 0$, da $\text{deg}(f) \leq n$ 
\end{enumerate}
\end{comment*}

\begin{theorem}
Sei $p$ das Interpolationspolynom zur Funktion $f$ in den Tschebyscheffknoten $x_0, \dots, x_n$ (Wurzeln von $T_{n+1}$), so gilt:
\begin{align*}
&p(x) = \frac{1}{2} c_0 + \sum_{j=1}^n c_j T_j(x), &\\
\intertext{wobei}
&c_k = \frac{2}{n+1} \sum_{l=0}^n f(x_l) \cos\left(k \frac{2l+1}{n+1}\frac{\pi}{2}\right), \quad \text{für $k=0,...,n$}
\end{align*}

\begin{proof}[Beweis]
Betrachte $(p, T_k)$
\begin{align*}
(p, T_k) &= \frac{1}{2} (T_0, T_k) + \sum_{l=1}^n c_l (T_l, T_k) &\\
&= \begin{cases}
c_k (T_k, T_k) & \text{für }k \neq 0 \\
\frac{1}{2} c_0 (T_0, T_0)  & \text{für }k = 0 
\end{cases}&\\
&\overset{(10.8)}{=} \frac{n+1}{2} c_k&\\
&= \frac{n+1}{2} \sum_{l=0}^n f(x_l) T_k(x_l) &\\
&= \frac{n+1}{2} \sum_{l=0}^n f(x_l) \cos\left(k \underbrace{\arccos\left(\cos\left(\frac{2l+1}{n+1}\frac{\pi}{2}\right)\right)}_{ = \frac{2l+1}{n+1}\frac{\pi}{2}}\right) &\\
\end{align*}
\end{proof}
\end{theorem}
$p(x)$ lässt sich als bei bekannten Koeffizienten $c_k$ leicht berechnen/auswerten.

\begin{theorem}[Clenshaw Algorithmus]
Sei $p \in \mathcal{P}_n$ durch die Koeffizienten $c_0, ..., c_n$ in der Form
\begin{align*}
&p(x) = \frac{1}{2} c_0 + \sum_{j=1}^n c_j T_j(x)&\\
\intertext{gegeben. Setzt man }
&d_{n+1} = d_{n+2} = 0 &\\
\intertext{und definiert für $x$}
&d_k = c_k + 2x d_{k+1} - d_{k+2}, \quad \text{für }k=n,n-1,...,1, 0&\\
\intertext{so gilt:}
&p(x) = \frac{1}{2} (d_0-d_2)&\\
\end{align*}
\begin{proof}[Beweis]
Verwende die Rekursionsformel aus (10.2) iii) ($T_{k+1} = 2x T_k + T_{k-1}$). Dann ist 
\begin{align*}
p(x) &= \frac{1}{2} c_0 + \sum_{l=1}^n c_l T_l(x) &\\
&= \frac{1}{2} c_0 + \sum_{l=1}^{n-3} c_l T_l(x) + c_{n-2} T_{n-2}(x) + c_{n-1} T_{n-1}(x) + c_{n} T_{n}(x) &\\
&= \frac{1}{2} c_0 + \sum_{l=1}^{n-3} c_l T_l(x) + (c_{n-2} - \underbrace{c_n}_{=d_n}) T_{n-2}(x) + \underbrace{(c_{n-1} + 2xc_n)}_{=d_{n-1}} T_{n-1}(x) &\\
&= \frac{1}{2} c_0 + \sum_{l=1}^{n-4} c_l T_l(x) + (c_{n-3} - d_{n-1}) T_{n-3}(x) + \underbrace{(c_{n-2} - d_n + 2xd_{n-1})}_{=d_{n-2}} T_{n-2}(x) &\\
\intertext{induktiv erhält man}
&= (\frac{1}{2} c_0 - d_2) \underbrace{T_0(x)}_{=1} + \underbrace{(c_1 - d_3 + 2xd_2)}_{=d_1} \underbrace{T_1(x)}_{=x} &\\
&= \frac{1}{2} (\underbrace{c_0 - 2d_1x - d_2}_{=d_0} -d_2) &\\
&= \frac{1}{2} (d_0-d_2)
\end{align*}
\end{proof}
\end{theorem}

\begin{comment*}
Bei der Verwendung von Rekursionen ist es wichtig zu verstehen, wie sich Rundungsfehler auswirken. 
\begin{description}
  \item[Beispiel:]
    $x_{n+1} = 10x_n - 9, \quad x_0 = 1$ \\
    $ \Rightarrow \forall n \in \mathbb{N}: \medspace x_n = 1$ \\
    Was passiert bei fehlerhafter Startwerten $\tilde{x}_0 = 1+ \varepsilon$? \\
    $ \tilde{x}_{n+1} = 10 \tilde{x}_n - 9, \quad \tilde{x}_n = 1+ 10^n \varepsilon$
\end{description}
Der Clenshaw-Algorithmus ist stabil, wie im Folgenden gezeigt wird:
\end{comment*}

\begin{theorem}
Für den Clanshaw-Algorithmus mit Fehlern $\varepsilon_k$ in der Rekursion, d.h. für 
\begin{align*}
&\tilde{d}_{n+1} = \tilde{d}_{n+2} = 0 &\\
&\tilde{d}_k = c_k + 2x \tilde{d}_{k+1} - \tilde{d}_{k+2} + \varepsilon_k, \quad k=n, n-1, ..., 0 &\\
\intertext{Dabei ist $\varepsilon_k$ der Rundungsfehler in der k-ten Iteration. Für
$\tilde{p}(x) = \frac{1}{2} (\tilde{d}_0 - \tilde{d}_2)$ gilt:}
& \vert \tilde{p}(x) - p(x) \vert \leq \sum_{j=0}^n \vert \varepsilon_j \vert, \quad \text{für } \vert x \vert < 1,
\intertext{wobei $p(x)$ mit (10.10) berechnet wird.}
\end{align*}

\begin{proof}[Beweis]
Setze $\varepsilon_k := \tilde{d}_k - d_k$ (für $d_k$ aus (10.10)). Dann gilt:
\begin{align*}
& \varepsilon_k = \varepsilon_k + 2x \varepsilon_{k+1} - \varepsilon_{k+2}, \quad \text{für } k=n,...,0 &\\
&\varepsilon_{n+1} = 0 \quad \text{und} \quad \varepsilon_{n+2} = 0
\end{align*}
Mit Satz (10.10) gilt für $c_k = \varepsilon_k$ und $d_k = \varepsilon_k$:
\begin{align*}
& \frac{1}{2} (\varepsilon_0 -\varepsilon_2) = \frac{1}{2} \varepsilon_0 + \sum_{j=1}^n \varepsilon_j T_j(x) 
\intertext{Da $\vert T_j(x) \vert \leq 1$ für $x \in [1, 1]$ gilt:}
& \vert \tilde{p}(x) - p(x) \vert \overset{\Delta-UGL}{\leq} \frac{1}{2} \vert \varepsilon_0 \vert + \sum_{j=1}^n \vert \varepsilon_j \vert  
\end{align*}
\end{proof}
\end{theorem}

\begin{comment*}
Die Approximation einer Funktion durch die Summe von Tschebyscheffpolynomen wird im Computer zur Berechnung von Funktionen wie log, exp, sin, cos,... verwendet.
\end{comment*}

\begin{example}
\underline{Ziel:} Berechne $\ln(x)$ für $0 \leq x_{\min} < x \leq x_{\max}$. $x_{\min}, x_{\max}$ ist die kleinste/größte positive darstellbare Zahl auf dem gegebenen Computer. \\
$x \medspace \text{"="} \medspace \underbrace{\left[ 1, b_1, b_2, ..., b_M \right]}_{\text{"Mantisse"}} * 2^N, \quad b_j \in \{0,1\}$\\
d.h. $ x = 2^N (1+ b_1 \frac{1}{2} + b_2 \frac{1}{4} + ... + b_M \frac{1}{2^M}) = 2^N (1+t), \quad t \in (0,1)$ \\
$\ln(x) = \ln(1+t) + N \underbrace{\ln(2)}_{\text{Konstante}}$ \\
Das Problem $\ln(x)$ zu berechnen ist damit auf das Problem $\ln(1+t)$ für $t \in [0,1]$ zu berechnen reduziert worden. \\
Tschebyscheffinterpolation: $[-1, 1] \rightarrow [0,1], \medspace x \mapsto t = \frac{1+x}{2} \quad (\Leftrightarrow x=2t-1)$ \\
Für den Interpolationsfehler gilt:
\begin{align*}
\ln(1+ \frac{1+x}{2}) -p(x) &= \underbrace{\prod_{j=0}^n (x-x_j)}_{=2^{-n} \text{ für Tschebyscheff}} \frac{1}{(n+1)!} \frac{(-1)^{n-1} (n-1)!}{(1+\frac{1+\xi}{2})^n} \left(\frac{1}{2}\right)^n, \quad \xi \in [-1, 1] &\\
\Leftrightarrow \quad \vert \ln(1+ \frac{1+x}{2}) -p(x) \vert &= \frac{1}{4^n} \frac{1}{(n+1)^n}
\end{align*}
Für n=15 ist $\frac{1}{4^n} \frac{1}{(n+1)^n} \leq 10^{-11}$ \\
Berechnet werden also $c_0,..., c_{15}$ (einmal für alle Zeiten):
\begin{align*}
c_0 &= 0.75290562... &\\
c_1 &= 0.34... &\\
c_2 &= -0.029... &\\
c_3 &= 0.0036... &\\
c_4 &= -0.00004 &\\
\vert c_k \vert &\leq 10^{-9}, \quad \text{für }k >10 &\\
\end{align*}
Beobachtung: $c_k$ werden schnell klein.\\
Um eine Genauigkeit von $10^{-8}$ (einfache Genauigkeit) zu erreichen, benötigt man nur $c_0,..., c_9$.\\
Die Auswertung mit dem Clenshaw-Algorithmus benötigen wir 10 Multiplikation (vgl. Taylor $\log(1+t) = \sum_{k=1}^{\infty} \frac{(-1)^k}{k}t^k$)
\end{example}

\subsection{Hermit\'{e}-Interpolation}

Gegeben sind $(x_i, y_i, y_i')_{i=0}^n, \quad x_i \in [a,b]$ paarweise verschieden. Gesucht ist ein Polynom $p \in \mathcal{P}$, sodass
\begin{align*}
&p(x_i) = y_i \quad \text{und } &\\
&p'(x_i) = y_i', \quad \text{für } i=0,...,n. &\\
\end{align*}
\underline{Idee:} Lasse $\varepsilon \rightarrow 0$ laufen im Newtonschema:

\begin{tabular}{lll}
 
$x_0$ & $y_0$\\
 & & $\delta y[x_0, x_0+\varepsilon] = \frac{(y_0 + \varepsilon y_0') - y_0}{(x_0 +\varepsilon) - x_0} = y_0'$\\
$x_0+\varepsilon$ & $y_0 + \varepsilon y_0'$\\
 & & $\delta y[x_0+\varepsilon, x_1] \underset{\varepsilon \rightarrow 0}{\rightarrow} \delta y[x_0,x_1]$\\
$x_1$ & $y_1$\\
 & & $\delta y[x_1, x_1+\varepsilon] = y_1'$\\
$x_1+\varepsilon$ & $y_1+\varepsilon y_1'$\\
 
\end{tabular} \\ \\
Newtonsche Interpolationsformel:
\begin{align*}
p_{\varepsilon}(x) = y_0 &+ (x-x_0) \delta y[x_0, x_0+\varepsilon] &\\
&+ (x-x_0)(x-(x_0 + \varepsilon)) \delta^2y[x_0, x_0 + \varepsilon, x_1] &\\
&+ \dots &\\
&+ \left( \prod_{j=0}^{n-1} (x-x_j)(x-(x_j + \varepsilon))(x-x_1) \dots \delta^{2n+1} y[x_0,\dots, x_1]\right)
\end{align*}
damit ist:
\begin{align*}
p_{\varepsilon}(x_i) &= y_i &\\
p_{\varepsilon}(x_i + \varepsilon) &= y_i + \varepsilon y_i' &\\
\Rightarrow \quad y_i' &= \frac{p_{\varepsilon}(x_i + \varepsilon) - p_{\varepsilon}(x_i)}{\varepsilon} \underset{MWS}{=} p'_{\varepsilon}(\xi_i), \quad \text{für } \xi_i \in [x_i, x_i+\varepsilon]
\end{align*}
Für $\varepsilon \rightarrow 0$ definieren wir
\begin{align*}
\delta^k y[x_0, x_0, x_1, x_1, ...] &:= \lim_{\varepsilon \rightarrow 0} \delta^k y[x_0, x_0+\varepsilon, x_1, x_1+\varepsilon, ...] &\\
\end{align*}
und
\begin{align*}
p(x) &:= \lim_{\varepsilon \rightarrow 0} p_{\varepsilon}(x) &\\
&= y_0 + (x-x_0) \underbrace{\delta y[x_0, x_0]}_{y_0'} + (x-x_0)^2 \delta^2 y[x_0, x_0, x_1] &\\
&+ (x-x_0)^2(x-x_1) \delta^3 y[x_0, x_0, x_1, x_1] &\\
&+ ... + \prod_{j=0}^{n-1}(x-x_j)^2 (x-x_n) \delta^{2n-1}y[x_0,x_0, ..., x_n, x_n] &\\
p(x_i) &= \lim_{\varepsilon \rightarrow 0} p_{\varepsilon}(x_i) = y_i &\\
p'(x_i) &= \lim_{\varepsilon \rightarrow 0} p'_{\varepsilon}(x_i) = \lim_{\varepsilon \rightarrow 0}(\xi_{i, \varepsilon}) = y_i'
\intertext{für $\xi_{i, \varepsilon} \in [x_i, x_i+\varepsilon]$}
\end{align*}
\underline{Schema:} \\
\begin{tabular}{llllll}
 
$x_0$ & $y_0$\\
 & & $y_0'$\\
$x_0$ & $y_0 $ & & $ \delta^2[x_0, x_0, x_1]$\\
 & & $\delta y[x_0,x_1]$ & & $ \delta^3[x_0, x_0, x_1, x_1]$\\
$x_1$ & $y_1$ & & $ \delta^2[x_0, x_1, x_1]$ && $\dots$\\
 & & $y_1'$ && $\dots$\\
$x_1$ & $y_1$ & & $ \delta^2[x_1, x_1, x_2]$ && $\dots$\\
 & & $\delta y[x_1, x_2]$ && $\dots$\\
$x_2$ & $y_2$ && $\dots$\\
 & & $y_2'$\\
$x_2$ & $y_2$\\
 
\end{tabular} \\ \\
\underline{Eindeutigkeit:} \\
Annahme: $\exists q \in \mathcal{P}_{2n+1}$ mit $q(x_i) = y_i)$, $q'(x_i) = y_i'$\\
Dann ist $q-p \in \mathcal{P}_{2n+1}$ \\
$q-p$ besitzt doppelte Nullstelle in $x_i$ \\
$q-p = c \prod(x-x_i)^2$, da $\text{deg}\left(\prod_{i=0}^n (x-x_i)^2 \right) = 2n+2$ \\
$\Rightarrow \medspace c=0 \quad \Rightarrow \medspace q=p$ \\\\
Damit ist der folgende Satz bewiesen.

\begin{theorem}
Zu gegebenen $(x_i, y_i, y_i')_{i=0}^n$ mit $x_i \neq x_j$, falls $i \neq j$ existiert ein eindeutiges Polynom $p \in \mathcal{P}_{2n+1}$ mit $p(x_i) = y_i$ und $p'(x_i) = y_i'$ ($i=0,\dots, n$). $p$ kann mit Hilfe des Newtonschen Differenzenschemas mit doppelten eingeschriebenen Nullstellen (Knoten) berechnet werden.
\end{theorem}

\begin{theorem}[vgl. Satz (9.1)]
Sei $f: [a,b] \rightarrow \mathbb{R}$ $(2n+2)$-mal stetig differenzierbar ($f \in \mathcal{C}^{2n+2}([a,b], \mathbb{R})$), seien $x_0,..., x_n$ paarweise verschieden und sei $p$ Hermit\'{e}polynom aus (11.1) zu $(x_i,y_i, y_i')_{i=0}^n$. Dann gilt:
\[ \forall x \in [a,b] \exists \xi \in [a,b]: \medspace f(x)-p(x) = \prod_{j=0}^n (x-x_j)^2 \frac{f^{(2n+2)} (\xi)}{(2n+2)!} \]

\begin{proof}[Beweis]
Betrachte $\varepsilon \rightarrow 0$ für $p_{\varepsilon}(x)$ in der Fehlerformel (9.1):
\begin{align*}
f(x)-p(x) &= \prod_{j=0}^n (x-x_j)(x-(x_j+\varepsilon)) \frac{f^{(2n+2)}(\xi_{\varepsilon})}{(2n+2)!}, \quad \text{für } \xi_{\varepsilon} \in [a,b]
\end{align*}
Sei $\xi$ ein Häufungspunkt von $\{\xi_{\varepsilon}, \varepsilon > 0\}$. Dann existiert eine Nullfolge $(\varepsilon_k)_{k\in \mathbb{N}}$ mit $\xi_{\varepsilon_k} \rightarrow \xi$ für $k \rightarrow \infty$. $\Rightarrow$
\begin{align*}
f(x)-p(x) &= \lim_{k \rightarrow \infty} (f(x) - p_{\varepsilon_k}(x)) &\\
&= \prod_{j=0}^n (x-x_j)^2 \frac{f^{(2n+2)}(\xi)}{(2n+2)!}
\end{align*}
\end{proof}
\end{theorem}

\subsection{Spline-Interpolation}
Spline ist engl. für Holz- oder Metallfeder.\\
\underline{Theorie:} stammt von Schoenenberg aus dem Jahr 1946\\
\underline{Idee:} Suche 'glatte' Funktion $s$ durch vorgegebene Punkte $(x_i, y_i)_{i=0}^n$
\begin{enumerate}
  \item[i)] $s(x_i)= y_i$ ($i=0,...,n$)'Interpolationseigenschaft'
  \item[ii)] $s$ muss mind. 2-mal stetig differenzierbar sein und $ \int_a^b (s''(x))^2dx $ soll minimal sein. 'glatt'
\end{enumerate}
Dadurch vermeidet man Oszillationen, wie sie bei der Polynominterpolation hohen Grades entstehen.\\
Wir suchen also eine Funktion $s$, sodass für $\varepsilon \in \mathbb{R}$ und $h \in \mathcal{C}^1([a,b], \mathbb{R})$, $h(x_i) = 0 \medspace (i=,...,n)$ und 
\begin{align*}
\int_a^b (s''(x))^2 dx &\overset{!}{\leq} \int_a^b \left( (s(x) + \varepsilon h(x))'' \right)^2 dx &\\
&= \int_a^b (s''(x) + \varepsilon h''(x))^2 dx &\\
&= \int_a^b (s''(x))^2 dx + 2\varepsilon \int_a^b s''(x)h''(x) dx + \underbrace{\varepsilon^2\int_a^b (h''(x))^2dx}_{ \geq 0 } 
\end{align*}
Obige Ungleichung ist erfüllt, falls 
$$ \forall h \in \mathcal{C}^2([a,b]) \medspace \text{mit }h(x_i) = 0: \medspace \int_a^b h''(x) s''(x) dx = 0$$
Dabei gilt:
$$ \int_a^b h''(x) s''(x) dx = \left[ s''(x) h'(x) \right]_{x=a}^b - \int_a^b s'''(x) h'(x)dx$$
Falls $s'''(x) = \alpha_i$ für $x \in [x_{i-1}, x_i]$, dann ist 
\begin{align*}
\int_a^b s'''(x) h'(x) dx &= \sum_{i=1}^n \alpha_i \int_{x_{i-1}}^{x_i} h'(x) dx &\\
&= \sum_{i=1}^n \alpha_i \left(\underbrace{h(x_i)}_{= 0} - \underbrace{h(x_{i-1}}_{= 0}\right) &\\
&= 0
\end{align*}
$\Rightarrow$ Forderung: $\left[s''(x) h'(x) \right]_{x=a}^b = s''(b)h'(b) - s''(a)h'(a) \overset{!}{=} 0$

\begin{theorem}
Seien $f, s \in \mathcal{C}^2 ([a,b], \mathbb{R})$ zwei Funktionen, die in $a=x_0 < x_1 < ... < x_n = b$ dieselben Werte annehmen, d.h. 
\begin{align*}
f(x_i) = s(x_i) \medspace &(i=0,...,n)
\quad \text{und } \quad 
s\,\rule[-2mm]{0.1mm}{5mm}_{\thinspace [x_{i-1}, x_i]} &\in \mathcal{P}_3 \quad \text{für } i=1,...,n
\intertext{Falls}
s''(a) [f'(a) - s'(a)] &= s''(b) [f'(b) - s'(b)], \quad (*) 
\intertext{so gilt:}
\int_a^b (s''(x))^2dx &\leq \int_a^b(f''(x))^2dx
\end{align*}
\begin{proof}[Beweis]
Obige Rechnung für $h=f-s$ und $\varepsilon = 1$, $h(x_i) = 0$ \\
$[s''(x) h'(x)]_{x=a}^b = 0 \Leftrightarrow (*)$
\end{proof}
\end{theorem}

\begin{comment}
Die Bedingung (*) kann erreicht werden durch 
\begin{enumerate}
  \item[a)] Vorgabe von $s'(a) = f'(a)$, $s'(b) = f'(b)$ \\
  Der dadurch bestimmte Spline heißt \textbf{eingespannter} Spline.
  \item[b)] Vorgabe von $s''(a) = 0 = s''(b)$ \\
  Der dadurch bestimmte Spline heißt \textbf{natürlicher} Spline. Dieser hat aber schlechtere Approximationseigenschaften. 
\end{enumerate}
\end{comment}

\begin{nothing}[Konstruktion des Splines]
Gegeben sind $(x_i, y_i)$ $i=0,...,n$, $a = x_0 < x_1 < ... < x_n = b$, $s\,\rule[-2mm]{0.1mm}{5mm}_{\thinspace [x_{i-1}, x_i]} =: s_i \in \mathcal{P}_3$. \\
Hermite-Interpolation:
\begin{align*}
&s_i(x_i) = y_i &\\
&s_i(x_{i-1}) = y_{i-1} &\\
&s_i'(x_i) = \tau_i &\\
&s_i'(x_{i-1}) = \tau_{i-1}
\end{align*}
Dabei sind $\tau$ unbekannte Steigungen. \\
Ansatz: 
\begin{align*}
s_i(x) &= y_{i-1} + (x-x_{i-1}) \delta y[x_{i-1}, x_i] + (x-x_{i-1})(x-x_i) \left( \alpha (x-x_{i-1}) + \beta (x-x_i) \right) &\\
s_i'(x_{i-1}) &= \delta y[x_{i-1}, x_i] + \beta (x_{i-1} - x_i)^2 = \tau_{i-1} &\\
s_i'(x_i) &= \delta y[x_{i-1}, x_i] + \alpha (x_i - x_{i-1})^2 = \tau_i &\\
\Rightarrow \alpha &= \frac{\tau_i - \delta y[x_{i-1}, x_i]}{(x_i - x_{i-1})^2} &\\
\beta &= \frac{\tau_{i-1} - \delta y[x_{i-1}, x_i]}{(x_{i-1}- x_i)^2} &\\
h_i &= x_i-x_{i-1} &\\
\Rightarrow s_i(x) &= y_{i-1} + (x-x_{i-1}) \delta y [x_{i-1}, x_i] &\\ 
&+ \frac{(x-x_{i-1} )(x-x_i)}{h_i^2} \left( (\tau_i - \delta y[x_{i-1},x_i])(x-x_{i-1})+(\tau_{i-1} - \delta y[x_{i-1},x_i])(x-x_i) \right) &\\
\end{align*}
Für beliebige $\tau_0,..., \tau_n$ erhalten wir $s: [a,b] \rightarrow \mathbb{R}$ mit 
\begin{enumerate}
  \item[i)] $s\,\rule[-2mm]{0.1mm}{5mm}_{\thinspace [x_{i-1}, x_i]} \in \mathcal{P}_3$
  \item[ii)] $s(x_i) = y_i$
  \item[iii)] $s \in \mathcal{C}^1([a,b])$
\end{enumerate} 
Bestimme $\tau_0,..., \tau_n$ so, dass $s \in \mathcal{C}^2([a,b])$, d.h. $s_i''(x_i) = s_{i+1}''(x_i)$ für $i=1,...,n-1$. Das sind $(n-1)$ Bedingungen. (\#) \\
Beim eingespannten Spline sind $\tau_0$ und $\tau_n$ bekannt und die $\tau_1, ..., \tau_{n-1}$ sind die Unbekannten. \\
Mit
\begin{align*}
&(fg)'' = f''g + 2f'g' + fg''
\intertext{gilt wegen}
&\frac{d^2}{dx^2} \left( (x-x_{i-1})^2 (x-x_i) \right) \rule[-2mm]{0.1mm}{5mm}_{\thinspace x = x_i} = 4h_i
\intertext{und}
&\frac{d^2}{dx^2} \left( (x-x_{i-1}) (x-x_i)^2 \right) \rule[-2mm]{0.1mm}{5mm}_{\thinspace x = x_i} = 2h_i
\intertext{folgendes:}
s_i''(x_i) &= \frac{1}{h_i^2} \left( (\tau_i - \delta y[x_{i-1},x_i]) 4h_i + (\tau_{i-1} - \delta y[x_{i-1},x_i]) 2h_i \right) &\\
&= \frac{2}{h_i} \left( 2 \tau_i - 3 \delta y[x_{i-1},x_i] + \tau_{i-1} \right)
\intertext{Ebenso zeigt man:}
s_{i+1}''(x_i) &= -\frac{2}{h_{i+1}} \left( 2 \tau_i - 3 \delta y[x_{i},x_{i+1}] + \tau_{i+1} \right)
\end{align*}
Die Bedingung (\#) $s_i''(x_i) = s_{i+1}''(x_i) \quad i=1,...,n-1$ wird damit zu
\begin{align*}
\frac{\tau_{i-1}}{h_i} + 2 \left( \frac{1}{h_i} + \frac{1}{h_{i+1}} \right) \tau_i + \frac{\tau_{i+1}}{h_{i+1}} &= 3 \left( \frac{\delta y[x_{i-1}, x_i]}{h_i} + \frac{\delta y[x_{i}, x_{i+1}]}{h_{i+1}}\right)&\\
\end{align*}
Damit erhalten wir ein LGS für $\tau_1, ..., \tau_{n-1}$ 
\begin{align*}
&\underbrace{
\begin{bmatrix} 
(\frac{2}{h_1} + \frac{2}{h_2}) & \frac{1}{h_2} &0&\dots&0\\ 
\frac{1}{h_2} & (\frac{2}{h_2} + \frac{2}{h_3}) & \frac{1}{h_3} &\ddots&\vdots\\ 
0& & \ddots &&0\\
\vdots&\ddots&&& \frac{1}{h_{n-1}} \\
0&\dots&0& \frac{1}{h_{n-1}} & ( \frac{2}{h_{n-1}} +  \frac{2}{h_{n}})\\
\end{bmatrix}
}_A
\underbrace{
\begin{bmatrix}
\tau_1 \\
\tau_2 \\
\vdots \\
\tau_{n-1} \\
\end{bmatrix}
}_{\tau} 
= 
\underbrace{
\begin{bmatrix}
3 \left( \frac{\delta y[x_0,x_1]}{h_1} + \frac{\delta y[x_1,x_2]}{h_2} \right) - \frac{\tau_0}{h_1} \\
3 \left( \frac{\delta y[x_1,x_2]}{h_2} + \frac{\delta y[x_2,x_3]}{h_3} \right)\\
\vdots \\
3 \left( \frac{\delta y[x_{n-2},x_{n-1}]}{h_{n-1}} + \frac{\delta y[x_{n-1},x_n]}{h_n} \right) - \frac{\tau_n}{h_n} \\
\end{bmatrix}
}_b
\end{align*}
\end{nothing}

\begin{theorem}
Sei $A$ wie in (12.3) und $A\tau = b$, dann gilt
$$\max_i \vert \tau_i \vert \leq \frac{h}{2} \max_i \vert b_i \vert,$$
wobei $\tau = (\tau_1, ..., \tau_{n-1})^T$, $b = (b, ..., b{n-1})^T$, $h = \max_i h_i$.

\begin{proof}[Beweis]
Sei $j \in \{1, ..., n-1\}$ so, dass $\vert \tau_j \vert = \max_i \vert \tau_i \vert$. Dann gilt:
\begin{align*}
2 \left( \frac{1}{h_j} + \frac{1}{h_{j+1}} \right) \tau_j &= -\frac{\tau_{j-1}}{h_j} -\frac{\tau_{j+1}}{h_{j+1}} + b_j &\\
\Rightarrow 2 \left\vert \frac{1}{h_j} + \frac{1}{h_{j+1}} \right\vert & \leq \left\vert \frac{\tau_{j-1}}{h_j} \right\vert + \left\vert \frac{\tau_{j+1}}{h_{j+1}} \right\vert + \vert b_j \vert &\\
& \leq \left( \frac{1}{h_j} + \frac{1}{h_{j+1}} \right) \vert \tau_j \vert + \max_i \vert b_i \vert &\\
\Rightarrow \left( \frac{1}{h_j} + \frac{1}{h_{j+1}} \right) \vert \tau_j \vert &\leq \max_i \vert b_i \vert &\\
\Rightarrow \max_i \vert \tau_i \vert &= \vert \tau_j \vert \leq \frac{h}{2} \max_i \vert b_i \vert 
\end{align*}
\end{proof}
\end{theorem}

\begin{korollar}
Die Matrix A aus (12.4) ist invertierbar.
\begin{proof}[Beweis]
Die einzige Lösung von $A\tau = 0$ ist $\tau = 0$, $0 \in \mathbb{R}^{n-1}$
\end{proof}
\end{korollar}

\begin{korollar}
Der eingespannte Spline existiert und ist eindeutig.
\begin{proof}[Beweis]
Folgt aus (12.5)
\end{proof}
\end{korollar}

\subsection{Fehler bei der Splineinterpolation}
Vorraussetzungen für diesen Abschnitt: \\
$a = x_0 < x_1 < ... < x_n = b$, \\
$h_i= x_i-x_{i-1}$, \\
$h:= \max_i \vert h_i \vert$

\begin{theorem}
Sei $f \in \mathcal{C}^4([a,b])$, $s$ der eingespannte Spline, d.h. $s'(a) = f'(a)$, $s'(b) = f'(b)$, $s(x_i) = f(x_i)$ für $i=0,...,n$. Dann gilt für $x\in [a,b]$
$$ \vert f(x) - s(x) \vert \leq \frac{5}{384} h^4 \max_{\xi \in[a,b]} \vert f^{(4)}(\xi) \vert $$
\begin{proof}[Beweis] Siehe (13.3)
\end{proof}
\end{theorem}

\begin{lemma}
Unter den Vorraussetzungen von (13.1) gilt für  $s'(x_i) = \tau_i$:
$$ \vert f'(x_i) - \tau_i \vert \leq \frac{h^3}{24} \max_{\xi \in [a,b]} \vert f^{(4)}(\xi) \vert$$
\begin{proof}[Beweis (für den äquidistanten Fall)]
Für $i=1, ..., n-1$ erfüllen die $\tau_i$ 
\begin{align*}
&\frac{1}{h} (\tau_{i-1} + 4 \tau_i + \tau_{i+1}) = \frac{3}{h^2} (f(x_{i+1}) - f(x_{i-1})) = b_i &\\
\intertext{Ersetze nun $\tau_i$ durch $f'(x_i)$, so gilt:}
&\frac{1}{h} (f'(x_{i-1}) + 4f'(x_i) + f'(x_{i+1})) - \frac{3}{h^2} ( f(x_{i+1}) - f(x_{i-1})) =: \delta_j &\\
\intertext{Taylorentwicklung von $f'(x_{i-1}), f'(x_{i+1}), f(x_{i-1}), f(x_{i+1})$ um $x_i$}
&f(x_{i+1}) = f(x_i + h) = f(x_i) + hf'(x_i) + \frac{h^2}{2!} f''(x_i) &\\
& + \frac{h^3}{3!} f'''(x_i) + h^4 \int_0^1 \frac{(1-t)^3}{3!} f^{(4)}(x_i+th) dt &\\
&f'(x_{i+1}) =  f'(x_i) + hf''(x_i) + \frac{h^2}{2!} f''(x_i) &\\
&+ h^3 \int_0^1 \frac{(1-t)^2}{2!} f^{(4)}(x_i+th) dt &\\
\intertext{und analog für $f(x_{i-1}) = f(x_i-h)$ und $f'(x_{i-1})$. $\Rightarrow$}
\delta_j &= \frac{1}{h} (f'(x_i) - hf''(x_i) + \frac{h^2}{2} f''(x_i) + R_{i-}' &\\
&\quad + 4 f'(x_i) + f'(x_i) + hf''(x_i) + \frac{h^2}{2} f'''(x_i) + R_{i+}') &\\
&\quad -\frac{3}{h^2} (f(x_i) + hf'(x_i) + \frac{h^2}{2}f''(x_i) + \frac{h^3}{3!} f'''(x_i) + R_{i+} &\\
&\quad- f(x_i) + hf'(x_i) - \frac{h^2}{2} f''(x_i) + \frac{h^3}{3!} f'''(x_i) + R_{i-}) &\\
&= h^2 \int_0^1 \left( \frac{(1-t)^2}{2!} - 3 \frac{(1-t)^3}{3!} \right) f^{(4)}(x+th) dt &\\
&\quad+ h^2 \int_0^1 \left( \frac{(1-t)^2}{2!} - 3 \frac{(1-t)^3}{3!} \right) f^{(4)}(x-th) dt &\\
&= h^2 \left( f^{(4)}(\xi_i) + f^{(4)}(\eta_i) \right) \int_0^1 \frac{(1-t)^2}{2} - \frac{(1-t)^3}{2} dt &\\
&=\frac{h^2}{24} \left(f^{(4)}(\xi_i) + f^{(4)}(\eta_i)\right), \quad \xi_i \in [x_{i-1},x_i], \medspace \eta_i \in [x_i, x_{i+1}] &\\
\Rightarrow \vert \delta_i \vert &\leq \frac{h^2}{12} \max_{\xi \in [a,b]} \vert f^{(4)}(\xi) \vert
\intertext{Definiere nun $e_i := f'(x_i) - \tau_i$ für $i=0,...,n$. Diese erfüllen die Bedingung $e_0 = 0, \medspace e_n = 0$ vom eingespannten Spline. Für $f' = (f'(x_1),..., f'(x_{n-1}))^T$, $\delta = (\delta_1, ..., \delta_{n-1})^T$ und $e = (e_1, ..., e_{n-1})$ gilt: $A \tau = b$ und $Af' = b+ \delta$. Mit (12.4) gilt dann}
\max_i \vert e_i \vert & \leq \frac{h}{2} \max_i \vert \delta_i \vert \leq \frac{h^3}{24} \max_{\xi \in [a,b]} \vert f^{(4)}(\xi) \vert 
\end{align*}
\end{proof}
\end{lemma}

\begin{nothing}[Beweis von (13.1)]
Für $x \in [x_{i-1}, x_i]$ ist $f(x) - s_i(x) = f(x) - p_i(x) + p_i(x) - s_i(x)$, \\
wobei $p_i$ das kubische Hermiteinterpolationspolynom zu $f$ ist mit \\
$p_i(x_i) = f(x_i)$, $p_i(x_{i-1}) = f(x_{i-1})$, $p_i'(x_i) = f'(x_i)$, $p_i'(x_{i-1}) = f'(x_{i-1})$\\
Nach Satz (11.2) gilt für ein $\xi \in [x_{i-1}, x_i]$:
\begin{align*}
\vert f(x) - p_i(x) \vert &= \vert (x-x_i)^2(x-x_{i-1})^2 \vert \left\vert \frac{f^{(4)}(\xi)}{24} \right\vert &\\
& \leq \frac{h^4}{16*24} \vert f^{(4)}(\xi) \vert = \frac{h^4}{384} \vert f^{(4)}(\xi) \vert
\end{align*}
Weiter gilt:
\begin{align*}
s_i(x) - p_i(x) &= (x-x_{i-1})(x-x_i) ((\tau_i - f'(x_i)) (x-x_{i-1}) &\\
& \quad +  (\tau_{i-1} - f'(x_{i-1})) (x-x_{i}) ) \frac{1}{h^2}
\intertext{Da $\frac{(x-x_{i-1})(x-x_i)}{h^2} \leq \frac{1}{4}$ für $x \in [x_{i-1}, x_i]$ gilt mit (13.2)}
\vert s_i(x) - p_i(x) \vert  &\leq \frac{1}{4} \frac{h^3}{24} \max_{\xi \in [a,b]} \left\vert f^{(4)}(\xi) \right\vert \underbrace{\left( \vert x-x_{i-1} \vert + \vert x-x_i \vert \right)}_{= h} &\\
& = \frac{h^4}{96} \max_{\xi \in [a,b]} \left\vert f^{(4)}(\xi) \right\vert &\\
\intertext{Ingesamt gilt also:}
\vert f(x) - s_i(x) \vert & \leq h^4 \frac{1+4}{384} \max_{\xi \in [a,b]} \left\vert f^{(4)}(\xi) \right\vert
\end{align*}
\qed
\end{nothing}

\begin{comment*}
Wie wirken sich Störungen/Fehler in den Daten auf den interpolierenden Spline aus? \\
Gegeben seien $(x_i, y_i)_{i=0}^n$ und $(y_0', y_n')$. Dadurch erhält man einen Spline $s(x)$. \\
Für Daten $(x_i, \tilde{y}_i)_{i=0}^n$ und $(y_0', y_n')$ erhält man einen Spline $\tilde{s}(x)$. Der Einfachheit halber sind $y_0'$ und $y_n'$ fehlerfrei. \\
Nun gilt:
$$s(x) - \tilde{s}(x) = \sum_{i=0}^n (y_i - \tilde{y}_i) l_i(x),$$
wobei $l_i(x)$ ein kubischer Spline mit 
$$l_i(x_j) =
\begin{cases}
  1 & \text{, falls } i=j \\
  0 &\text{, sonst} \\
\end{cases}  $$
und $l_i'(a) = 0 = l_i'(b)$ ist ("Lagrange-Spline"). \\
Diese zeigen keine Oszillationen wie Lagrangepolynome auf äquidistanten Stützstellen. Es gilt 
$$\max_{x \in [a,b]} \vert s(x) - \tilde{s}(x) \vert \leq \Lambda_n \max_i \vert y_i - \tilde{y}_i \vert $$
mit der Spline Lebesguekonstante 
$$\Lambda_n = \max_{x \in [a,b]} \sum_{i=0}^n \vert l_i(x) \vert $$
\underline{Ohne Beweis:} Für äquidistante Verteilungen gilt für Splines $\forall n \in \mathbb{N}: \medspace \Lambda_n \leq 2$
\end{comment*}

\subsection{Numerische Differentiation}
\begin{description}
  \item[Problemstellung:] Zu $f: [a,b] \rightarrow \mathbb{R}$ berechne näherungsweise $f'(x)$ für $x \in [a,b]$:
  $$f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}$$
  Falls $f \in \mathcal{C}^2([a,b])$ gilt:
  \begin{align*}
  f(x+h) &= f(x) + hf'(x) + \frac{h^2}{2} f''(\xi), \quad \text{für } \xi \in [a,b] &\\
  \Rightarrow \frac{f(x+h) - f(x)}{h} &= f'(x) + \frac{h}{2} f''(\xi)
  \end{align*}
  Allerdings ist ein Grenzübergang $h \rightarrow 0$ auf einem Computer problematisch, da statt $\frac{f(x+h) - f(x)}{h}$ nur $\frac{f(x+h) - f(x) + \varepsilon}{h}$ berechnet werden kann für ein $\varepsilon < \text{eps}$ (Maschinengenauigkeit) \\
  $\text{eps} \approx 10^{-16}$
  \item[Idee:] Um $f'$ zu approximieren, ersetze $f$ durch ein Polynom $p$ oder ein Spline $s$ und approximiere $f'$ durch $s'$ oder $p'$.
  \item[Berechnung von $p'(x)$:]  Dividierte Differenzen:\\
  \begin{tabular}{lllllll}
    $x$ & $p(x) = b_0$ \\
    &&$b_1$\\
	$x_0$ & $y_0 = f(x_0)$ && $b_2$\\
	 & & $\delta^1y[x_0, x_1]$&& $b_3$\\
	$x_1$ & $y_1 = f(x_1)$ & &$\delta^2y[x_0, x_1, x_2]$  &&$\ddots$\\
	 & & $\delta^1y[x_1, x_2]$ & & $\delta^3y[x_0, x_1, x_2, x_3]$&& $b_n = \delta^ny[x_0,...,x_n]$\\
	$x_2$ & $y_2 = f(x_2)$ & & $\delta^2y[x_1, x_2, x_3]$\\
	 & & $\delta^1y[x_2, x_3]$\\
	$x_3$ & $y_3 = f(x_3)$ \\
	$\vdots$ & $\vdots$\\
	$x_n$ & $y_n = f(x_n)$
  \end{tabular}\\\\
  Interpolationspolynom $p \in \mathcal{P}_n$:
  \begin{align*}
    p(x) &= \sum_{i=0}^n \prod_{j=0}^{i-1} (x-x_i) \delta^iy[x_0,..., x_i] &\\
    &= x^n \delta^ny[x_0,..., x_n] + r, \quad \text{für }r \in \mathcal{P}_{n-1} &\\
    p^{(n)} &= n! \delta^ny[x_0,..., x_n]
  \end{align*}
  Füge weitere Diagonale zu Knoten $x$ in obigem Schema hinzu mit $b_0 = p(x)$ und $b_k = \delta^k y[x, x_0, x_1, ..., x_{k-1}]$. Nach Definition ist 
  $$b_{k+1} = \frac{b_k - \delta^ky[x_0,..., x_k]}{x-x_k}$$
  Rechne nun im Newtonschema von rechts nach links (da $b_n = \delta^ny[x_0,..., x_n]$).\\
  $b_n = \delta^ny[x_0,...,x_n]$ für $k= n-1, ..., 0$.\\
  $b_k = b_{k+1} (x-x_k) + \delta^ky[x_0,...,x_k]$ \\
  $p(x) = b_0$\\
  Nach dem Hornerschema.\\
  Berechne nun die Ableitungen: \\
  Füge weitere Diagonale zu Knoten $x+ \varepsilon$ hinzu und lasse $\varepsilon \rightarrow 0$ laufen \\
  \begin{tabular}{lllllll}
    $x+ \varepsilon$ & $p(x + \varepsilon) = c_0$ \\
    && $c_1 = p'(x)$ \\
    $x$ & $p(x) = b_0$ \\
    &&$b_1$\\
	$x_0$ & $y_0 = f(x_0)$ && $b_2$&&$\ddots$\\
	 & & $\delta^1y[x_0, x_1]$&& $b_3$&&$c_n$\\
	$x_1$ & $y_1 = f(x_1)$ & &$\delta^2y[x_0, x_1, x_2]$  &&$\ddots$ & =\\
	 & & $\delta^1y[x_1, x_2]$ & & $\delta^3y[x_0, x_1, x_2, x_3]$&& $b_n$\\
	$x_2$ & $y_2 = f(x_2)$ & & $\delta^2y[x_1, x_2, x_3]$&&& =\\
	 & & $\delta^1y[x_2, x_3]$&&&& $\delta^ny[x_0,...,x_n]$\\
	$x_3$ & $y_3 = f(x_3)$ \\
	$\vdots$ & $\vdots$\\
	$x_n$ & $y_n = f(x_n)$
  \end{tabular}\\\\
  \underline{Algorithmus zur Berechnung von $p'(x)$:}
  \begin{algorithmic}
  \STATE $c_n = b_n$
  \FOR{$k = n-1, ...,1$}
  	\STATE $c_k = b_k + (x-x_{k-1})c_{k+1}$
  \ENDFOR
  \STATE $p'(x) = c_1$
  \end{algorithmic}
\end{description}

\begin{theorem}
Sei $f \in \mathcal{C}^{n+2}([a,b])$, $p$ Interpolationspolynom zu $f$ in $x_0,..., x_n \in [a,b]$ paarweise verschieden ($p\in \mathcal{P}_n$). \\
$\forall x \in [a,b] \medspace \exists \xi, \xi' \in [a,b]:$ 
$$f'(x) - p'(x) = \left( \sum_{i=0}^n \prod{j=0, \medspace j \neq i}^n (x-x_j) \frac{f^{(n+1)(\xi)}}{(n+1)!}\right) + \prod_{j=0}^n (x-x_j) \frac{f^{(n+2)}(\xi')}{(n+2)!}$$
\begin{proof}[Beweisskizze] (vgl. 9.1) \\
Sei \={x} fest aber beliebig, $\bar{p}$ das Hermiteinterpolationspolynom zu 
\begin{description}
  \item $\bar{p}(x_i) = f(x_i), \quad i=0,...,n$
  \item $\bar{p}(x) = f(x),$
  \item $\bar{p}'(x) = f'(x)$
\end{description}
Newtonschema und Newtoninterpolationspolynome liefert das Ergebnis.
\end{proof}
\end{theorem}

\section{Lineare Gleichungssysteme und lineare Ausgleichsrechnung}
%
\begin{description}
  \item[Ziele:]
    \begin{itemize}
      \item Berechne $x \in \mathbb{R}^n$, welches Lösung von $Ax = b$ ist, wobei $A \in \mathbb{R}^{n\times n}$ invertierbar und $b \in \mathbb{R}^n$.
      \item Berechne $x \in \mathbb{R}^m$, welches Lösung von $\min_{x\in \mathbb{R}^m} \Vert Ax - b \Vert _2$ ist, wobei $A \in \mathbb{R}^{n\times m}$, $b \in \mathbb{R}^n$ und $n>m$.
    \end{itemize}
\end{description}

\subsection{Gaußelimination}
%
\begin{example}\leavevmode
\begin{enumerate}
  \item[a)] Splineinterpolation $A\tau = b$, $A$ tridiagonal und symmetrisch.
  \item[b)] Computertomographie \\
    %\includegraphics[width=11cm]{Bild1.png}\\
    $\Delta I$: gemessener Intensitätsunterschied zwischen Quelle und Detektor \\
    $\Delta I = \int_{[a,b]} \alpha(x) dx$\\
    Dabei ist $\alpha(x)$ der Absorptionskoeffizient\\
    Annahme: $\alpha$ ist konstant in jeder Volumenzelle (Voxel) $\Rightarrow$\\
    $\Delta I = \sum_{j \in \text{Voxel}} \alpha_j l_j$\\
    $l_j$: Länge des Weges $[a,b]$ in Voxel $j$\\
    Viele Strahlen: $L_j(t) = \omega ( \varphi_j ) s_j + \omega^{\perp} ( \varphi_j ) t$\\
    %\includegraphics[width=8cm]{Bild2.png}\\
    $\omega(\varphi_j) = (\cos(\varphi_j), \sin(\varphi_j))$\\
    $\omega^{\perp}(\varphi_j) = (-\sin(\varphi_j), \cos(\varphi_j))$ \\\\
    $\begin{bmatrix} 
    l_{11} & l_{12} & \dots & l_{1M} \\
    \vdots & \vdots & & \vdots \\
    l_{N1} & l_{N2} & \dots & l_{NM} \\
    \end{bmatrix}
    \begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_M \\
    \end{bmatrix}
    =
    \begin{bmatrix}
    \Delta I_1 \\
    \vdots \\
    \Delta I_N \\
    \end{bmatrix}$\\\\
    $M$ ist dabei die Anzahl der Voxel.\\
    $l_{ij}$: Länge des i-ten Strahl im j-ten Voxel\\
    $\alpha_j$: Absorption im j-ten Voxel \\
    $\Delta I_i$: Intensitätsunterschied entlang vom Strahl i
\end{enumerate}
\end{example}

\begin{nothing}[Herleitung des Verfahrens (Wdh. LA)]
$Ax = b$, $A = (a_{ij})_{i,j = 1}^n$, $b = (b_i)_{i=1}^n$\\\\
%
$a_{11} x_1 + a_{12}x_2 + \dots + a_{1n}x_n = b_1$\\
$a_{21} x_1 + a_{22}x_2 + \dots + a_{2n}x_n = b_2$\\
$\vdots$ \\
$a_{n1} x_1 + a_{n2}x_2 + \dots + a_{nn}x_n = b_n$\\\\
%
Ohne Einschränkungen sei $a_{11} \neq 0 $. Da $A$ invertierbar ist, ist mindestens ein Element aus $\{a_{i1}, i=1,...,n\}$ ungleich 0. Man kann also Zeilen/Gleichungen so vertauschen, dass $a_{11} \neq 0$. Für $i=2,3,...,n$ multipliziere die 1-te Zeile mit $l_{i1} := \frac{a_{i1}}{a_{11}}$ und ersetze die i-te Zeile durch $\text{(i-te Zeile)} - l_{i1} * \text{(1-ste Zeile)}$. Dann ergibt sich folgendes Gleichungssystem: \\\\
%
$a_{11}^{(1)} x_1 + a_{12}^{(1)}x_2 + \dots + a_{1n}^{(1)}x_n = b_1^{(1)}$\\
$0 + a_{22}^{(1)}x_2 + \dots + a_{2n}^{(1)}x_n = b_2^{(1)}$\\
$\vdots$ \\
$0+ a_{n2}^{(1)}x_2 + \dots + a_{nn}^{(1)}x_n = b_n^{(1)}$\\\\
%
Dabei ist\\
$a_{1j}^{(1)} = a_{1j}$ für $j=1,...,n$, \\
$b_1^{(1)} = b_1$, \\
$(a_{i1}^{(1)} = 0)$\\
$a_{ij}^{(1)} = a_{ij} - l_{i1} a_{1j}$,\\
$b_i^{(1)} = b_i - l_{i1} b_1$ für $i=2,...,n$, $j=1,...,n$.\\\\
%
Da die $(n-1)\times (n-1)$ Untermatrix $A^{(1)} (2:n, 2:n)$ ebenfalls invertierbar ist, wiederholt man den eben beschriebenen Schritt.\\
Nach eventuellem Zeilentausch ist $a_{22}^{(1)} \neq 0$
%
\begin{align*}
l_{i2} &:= \frac{a_{i2}^{(1)}}{a_{22}^{(1)}}, \quad i=3,...,n &\\
b_2^{(2)} &= b_2^{(1)}, \quad a_{2j}^{(2)} = a_{2j}^{(1)}, \quad j=2,...,n &\\
b_i^{(2)} &= b_i^{(1)} - l_{i2} b_2^{(1)}, \quad i=3,...,n &\\
a_{ij}^{(2)} &= a_{ij}^{(1)} - l_{i2} a_{2j}^{(1)}, \quad i=3,...,n, \medspace j=2,...,n &\\
\end{align*}
%
Damit entsteht eine Folge\\ 
$(A, b), (A^{(1)}, b^{(1)}), (A^{(2)}, b^{(2)}),..., (A^{(n-1)}, b^{(n-1)}) =: (R,c)$\\
für eine obere Dreiecksmatrix $R$ (d.h. alle Einträge unter der Diagonalen sind 0).\\
Das Gleichungssystem mit ($r_{ii} \neq 0$)\\
%
\begin{align*}
r_{11}x_1 + r_{12}x_2 + \dots + r_{1n}x_n &= c_1 &\\
r_{22}x_2 + \dots + r_{2n}x_n &= c_2 &\\
\vdots& &\\
r_{nn}x_n &= c_{n} &\\
\end{align*}
%
Dabei ist\\
$x_n = \frac{c_n}{x_{nn}}$\\
$x_i = \frac{1}{r_{ii}}(c_i - \sum_{j=i+1}^n r_{ij}x_j)$ für $i=n-1,...,1$
\end{nothing}

\begin{theorem}
Für eine invertierbare Matrix $A \in \mathbb{R}^{n\times n}$ liefert das in (15.2) beschriebene Verfahren 
$$PA = LR,$$
wobei 
$$L = 
\begin{bmatrix} 
1 &&& 0 \\
l_{21} & \ddots \\
\vdots &\ddots& \ddots \\
l_{n1} &\dots& l_{n(n-1)}& 1 \\
\end{bmatrix}$$
$$R = 
\begin{bmatrix} 
r_{11} & r_{12} & \dots & r_{1n} \\
& \ddots &  & \vdots \\
\\
0&&& r_{nn} \\
\end{bmatrix}$$
und $P$ eine Permutationsmatrix ist.
\begin{proof}[Beweis]
Nehme an, dass die notwendige Zeilenvertauschungen bereits durchgeführt wurden, d.h. ersetze $A$ durch $PA$ (Zeilen und Spalten von $P$ bestehen aus kanonischen Einheitsvektoren z.B. $ P = \begin{bmatrix} 0&1&0&\\1&0&0&\\0&0&1&\\ \end{bmatrix}$)\\
Bezeichne mit $L_i \in \mathbb{R}^{n\times n}$:
$$L_i = 
\begin{bmatrix}
1 &&&&&0\\
& \ddots \\
&& 1 \\
&& -l_{i+1,i} &  \\
&& \vdots &&\ddots\\
&& \underbrace{-l_{n,i}}_{\text{i-te Spalte}} &&&1 \\
\end{bmatrix}$$
Damit ist
\begin{align*}
A^{(1)} &= L_1 A, \quad a_{ij}^{(1)} = a_{ij} - l_{i1} a_{1j} &\\
A^{(k)} &= L_k A^{(k-1)}, \quad k = 2,...,n-1 &\\
R &= A^{(n-1)} = L_{n-1}L_{n-2} ... L_1 A &\\
\Rightarrow A &= \underbrace{L_1^{-1}...L_{n-2}^{-1}L_{n-1}^{-1}}_{=L} R
\end{align*}
Setzt man 
$$V_i = 
\begin{bmatrix}
0 \\
& \ddots \\
&& 0 \\
&& l_{i+1,i} &  \\
&& \vdots &&\ddots&\\
&& \underbrace{l_{n,i}}_{\text{i-te Spalte}} &&&0 \\
\end{bmatrix}
\in \mathbb{R}^{n\times n}$$
so ist $L_i = I_n - V_i$, da $V_iV_k = 0$ für $i \leq k$.\\\\
$\underbrace{(I_n - V_i)}_{=L_i} (I_n + V_i) = I_n + V_i - V_i + \underbrace{V_iV_i}_{=0} = I_n$\\\\
d.h. $L_i^{-1} = I_n + V_i$.\\
Damit folgt $L = L_1^{-1}L_2^{-1} ... L_{n-1}^{-1} = (I_n + V_1)(I_n + V_2) ... (I_n + V_{n-1}) = I_n + V_1 + V_2 +... + V_{n-1} + \underbrace{V_1V_2 + ...}_{=0} = L$\\
Das schließende $L$ ist dabei das $L$ aus (15.2).
\end{proof}
\end{theorem}

\begin{comment*}
\begin{align*}
\det(PA) &= \det(P)\det(A) = (-1)^{\text{\# Vertauschungen}} \det(A) &\\
\det(PA) &= \det(LR) = \underbrace{\det(L)}_{=1} \det(R) = \prod_{i=1}^n r_{ii}
\end{align*}
\end{comment*}

\begin{nothing}[Vorwärts- und Rückwärts-Substitution]
Sobald man die LR-Zerlegung (lu-decomposition) von $A$ kennt, löst man $Ax = b$ wie folgt:
$$Pb = PAx = L\underbrace{Rx}_{=: c}$$
Löse $Lc = Pb$ ("Vorwärtssubstitution"):
%
\begin{algorithmic}
\STATE $c_1 = (Pb)_1$
\FOR{$i=2,...,n$}
\STATE $c_i = (Pb)_i - \sum_{j=1}^{i-1} l_{ij}c_j$
\ENDFOR
\end{algorithmic}
%
und anschließend:\\
Löse $Rx = c$ wie in (15.2) angegeben ("Rückwärtssubstitution").
\end{nothing}

\begin{nothing}[Aufwand]
Beim Schritt $A \rightarrow A^{(1)}$ benötigt man 
\begin{itemize}
  \item $(n-1) \in \mathcal{O}(n)$ Divisionen 
  \item $(n-1)^2 \in \mathcal{O}(n^2)$ Multiplikationen
  \item $(n-1)^2 \in \mathcal{O}(n^2)$ Additionen
\end{itemize}
Also insgesamt Operationen aus $\mathcal{O}(n^2)$.\\
$A^{(1)} \rightarrow A^{(2)}$: $(n-1)^2$ Operationen. \\
$A^{(2)} \rightarrow A^{(3)}$: $(n-2)^2$ Operationen. \\
$\vdots$\\
$A \rightarrow L,R$: $\sum_{j=1}^n j^2 \approx \frac{1}{3}n^3 \medspace \left(\in \mathcal{O}(n^3)\right)$, da $\underbrace{\frac{1}{n} \sum_{j=0}^n (\frac{j}{n})^2}_{ \approx \int_0^1 x^2 = \frac{1}{3}} n^3$\\
Die Lösung von $Lc = Pb$ kostet ebenso wie die Lösung von $Rx = c$ \\
$1+2+...+(n-1) \approx \frac{1}{2} n^2$ Operationen.\\
Der Hauptaufwand steckt also in der Berechnung der Zerlegung $PA = LR$. 
\end{nothing}

\begin{comment*}[Einschub zur Gleitkommarechnung (floating point arithmetic)]
Jeder reelle Zahl $0 \neq x$ kann für festes $B \in \mathbb{N}$, $B \geq 2 $ eindeutig durch
$$x = \pm \thinspace m \thinspace B^e$$
dargestellt werden, wobei $m \in [1, B)$, die Mantisse, $e \in \mathbb{Z}$, der Exponent und $B$ die Basis ist. \\
Durch den Computer kommen folgende Einschränkungen hinzu:
\begin{itemize}
  \item Es stehen nur $l$ Ziffern für die Mantisse $m$ zur Verfügung $\rightarrow$ $m$ wird gerundet.
  \item Es stehen nur $r$ Ziffern für den Exponenten zur Verfügung
\end{itemize}
%Beginne mit den Sätzen, Definitionen,...
\begin{description}
  \item[Definition] $\thinspace$\\
    Eine $l$-stellige-Basis-$B$-Gleitkommazahl mit Exponentialbereich $[e_{\min}, e_{\max}]$ ist ein Tripel $(\sigma, m, e)$. Dabei ist 
    \begin{itemize}
      \item $\sigma$ das Vorzeichen
      \item $m$ eine $l$-stellige Zahl zur Basis $B$ mit festgelegter Kommastelle
      \item $e$ die ganze Zahl in $[e_{\min}, e_{\max}]$
    \end{itemize}
    Der Wert von $(\sigma, m, e)$ ist $\sigma * m * B^e$.
  %
  \item[Beispiel] $\thinspace$\\
    Betrachte den Standard IEEE 754.\\
    Dieser stellt eine Zahl mit einfacher Genauigkeit dar zur Basis $B=2$ mit $l=32$ und $[e_{\min}, e_{\max}] = [-128, 127]$.
%    \begin{figure}[!htb]
%      \centering
%      \includegraphics[width=10cm]{Bild3.png}
%      \caption{From https://de.wikipedia.org/wiki/IEEE\_754}
%    \end{figure}\\
    Der Wert lässt sich berechnen aus
    $$(-1)^{\sigma} * (1,m) * 2^{\sum_{j} e_j^{2^j} - 127}$$
    Dabei ist $m$ binär dargestellt.
  %
  \item[Definition] $\thinspace$ \\
    Für eine reelle Zahl $x$ bezeichnen wir mit $fl(x)$ eine $l$-stellige-Basis-10-Darstellung von $x$ mit unbeschränktem Exponenten $e$, sodass 
    $$fl(x) = \pm \thinspace m \thinspace 10^e,$$
    wobei $m$ eine Zahl mit $l$ Stellen ist.\\
    Die \underline{Maschinengenauigkeit} $\text{eps}$ ist die kleinste positive Zahl, sodass $fl(1+ \text{eps}) > 1$. Also ist $\text{eps}$ der Abstand zwischen zwei benachbarten Mantissen.
  %
  \item[Beispiele] $\thinspace$ 
    \begin{description}
      \item Dezimalsystem ($B=10$) $\Rightarrow \quad \text{eps} = 5 * 10^{-e}$
      \item Binärsystem ($B=2$) $\quad \medspace \medspace  \thinspace \Rightarrow \quad \text{eps} = 2^{-e}$
    \end{description}
\end{description}
\end{comment*}

\subsection{Wahl des Pivotelements}

\begin{example}
\begin{align*}
10^{-4}x_1 + x_2 &= 1 &\\
x_1 + x_2 &= 2 &\\
\end{align*}
exakte Lösung:
\begin{align*}
x_1 = \frac{1}{0.9999} &= 1.0001\overline{0001} &\\
x_2 = \frac{0.9998}{0.9999} &= 0.9998\overline{9998} &\\
\end{align*}
bei dreistelliger dezimaler Gleitkommaartihmetik (Mantissenlänge 3, Basis 10)
\begin{align*}
0.100 * 10^{-3} x_1 + 0.100 * 10^1 x_2 &= 0.199 * 10^1 &\\
0.100 * 10^1 x_1 + 0.100 * 10^1 x_2 &= 0.200 * 10^1
\end{align*}
und damit erhält man für 
\begin{enumerate}
  \item [a)] $a_{11} = 10^{-4}$ (Pivot) \\
    \begin{align*}
    l_{21} &= \frac{a_{21}}{a_{11}} = 10^4 = 0.100 * 10^5 &\\
    a_{22}^{(1)} &= 0.100 * 10^1 - 0.100 * 10^5 = -0.100 * 10^5 &\\
    b_2^{(1)} &= 0.200 * 101 - 0.100 * 10^5 = -0.100 *10^5 &\\
    \end{align*}
    Aus $-0.100 * 10^5 x_2 = -0.100 *10^5$ folgt $x_2 = 0.100 * 10^1 = 1$ \\
    $\Rightarrow$ $x_1 = \frac{b_1 - a_{12}x_2}{a_{11}} = \frac{0.100 * 10^1 - 0.100 * 10^1 x_2}{0.100 * 10^1} = 0$
  \item [b)] Wähle Pivot $a_{21} = 1$:
    \begin{align*}
    x_1 + x_2 &= 2 &\\
    10^{-4} x_1 + x_2 &= 1 &\\
    \end{align*}
    ... $l_{21} = 10^{-4} \Rightarrow x_2 = 1, \medspace x_1 = 1$
\end{enumerate} 
\begin{description}
  \item[Erläuterung:] $\thinspace$ \\
    Falls $\vert l_{21} \vert$ groß ist, ergibt sich
    \begin{align*}
    a_{22}^{(1)} &= a_{22} - l_{21} a_{12} \approx l_{21} a_{12} &\\
    b_2^{(1)} &= b_2 - l_{21} b_1 \approx l_{21}b_1 &\\
    x_2 &= \frac{b_2^{(1)}}{a_{22}^{(1)}} \approx \frac{b_1}{a_{21}} &\\
    \end{align*}
    Bei der Berechnung von $x_1$ kommt es zu einer Stellenauslöschung $x_1 = {(b_1 - \underbrace{a_{12} x_2}_{b_1})}:{a_{11}} \approx 0$\\
    \underline{Ausweg:} Zeilentausch, sodass $\vert a_{21} \vert \leq \vert a_{11} \vert $. Dann ist $\vert l_{21} \vert \leq 1$.\\
    Spaltenpivotsuche: \\
    Nehme Pivotelement im $(k+1)$-ten Schritt
    $$a_{j(k+1)}^{(k)} \quad \text{mit} \quad \vert a_{j(k+1)}^{(k)} \vert = \max_{i=k+1,...,n} \vert a_{i(k+1)}^{(k)} \vert, $$
    d.h. das betragsmäßig größte Element der $(k+1)$-ten Spalte von $A^{(k)}$ unterhalb der Diagonalen inklusive des Diagonalelements. \\
%    \begin{figure}[!htb]
%    \centering
%    \includegraphics[width=6cm]{Bild4.png}
%    \end{figure}\\
    Damit erreicht man
    $$ \vert l_{i, (k+1)} \vert = \frac{\vert a_{i, k+1}^{(k)} \vert }{ \vert a_{k+1, k+1}^{(k)} \vert} \leq 1, \quad i=k+2,...,n$$
\end{description}
\end{example}

\subsection{Cholesky-Zerlegung für symmetrische positiv definite Matrizen}

\begin{definition}
Eine Matrix $A = (a_{ij})_{i,j=1}^n \in \mathbb{R}^{n\times n}$ heißt symmetrisch, falls $\forall i,j = 1,...,n: \medspace a_{ij} = a_{ji}$, d.h. $A = A^T$.\\
Eine Matrix $A$ ist positiv definit, falls 
$$\forall x \in \mathbb{R}^n: \medspace x^T Ax > 0$$
\end{definition}

\begin{theorem}
Sei $A$ symmtrisch positiv definit (kurz: spd), $A \in \mathbb{R}^{n\times n}$. Dann gilt:
\begin{enumerate}
  \item[i)] Die Gaußelimination kann ohne Zeilenvertauschungen durchgeführt werden
  \item[ii)] Für die Zerlegung $A=LR$ gilt $R = DL^T$ für eine Diagonalmatrix 
  $$D =
  \begin{bmatrix}
  r_1 && 0 \\
  & \ddots \\
  0 && r_{nn} \\
  \end{bmatrix}, \quad \text{wobei }\forall i = 1,...,n: \medspace r_{ii} > 0$$ 
\end{enumerate}
\begin{proof}[Beweis]
Es gilt: $a_{11} = e_{1}^T A e_1 > 0$, da $A$ spd, wobei $e_1 = (1,0,...,0)^T \in \mathbb{R}^n$ der 1. kanonische Basisvektor ist. Also ist $a_{11}$ ein möglicher Pivot. Schreibe $A$ nun als:
$$\left[
\begin{array}{c|c}
a_{11} & z^T \\
\hline
z & C \\
\end{array}
\right]$$
wobei $z = (a_{21}, ..., a_{n1})^T \in \mathbb{R}^{n-1}$ und $C$ eine symmetrische $(n-1)\times(n-1)$-Matrix ist. \\
Nun ist 
$$A^{(1)} = \left[
\begin{array}{c|c}
a_{11} & z^T \\
\hline
0 & C^{(1)} \\
\end{array}
\right]$$
$C^{(1)}$ ist symmetrisch, da 
$$c_{ij}^{(1)} = a_{ij} - \underbrace{\frac{a_{i1}}{a_{11}}}_{= l_{i1}} a_{1j} = a_{ji} - \underbrace{\frac{a_{j1}}{a_{11}}}_{= l_{j1}} a_{1i} = c_{ji}^{(1)}$$
Also ist $C^{(1)}$ insbesondere spd., da für 
$\left(
\begin{array}{c}
x_1 \\
\hline
\vdots \\
y \\
\vdots \\
\end{array}
\right) \neq 0 $ gilt:
$$ 0 < 
\left(
\begin{array}{c}
x_1 \\
\hline
\vdots \\
y \\
\vdots \\
\end{array}
\right)^T
A
\left(
\begin{array}{c}
x_1 \\
\hline
\vdots \\
y \\
\vdots \\
\end{array}
\right) 
=
\left(
\begin{array}{c}
x_1 \\
\hline
\vdots \\
y \\
\vdots \\
\end{array}
\right) 
\left[
\begin{array}{c|c}
a_{11} & z^T \\
\hline
z & C \\
\end{array}
\right]
\left(
\begin{array}{c}
x_1 \\
\hline
\vdots \\
y \\
\vdots \\
\end{array}
\right) 
= a_{11} x_1^2 + \underbrace{y^Tzx_1 + x_1 z^Ty}_{=2x_1y^Tz} +y^T Cy$$
Weiter ist:
$$ y^T C^{(1)} y = y^TCy - \frac{1}{a_{11}} y^Tzz^Ty = y^TCy - \frac{1}{a_{11}}(y^Tz)^2$$
Wählt man nun $x_1 = -\frac{y^Tz}{a_{11}}$, so gilt:
\begin{align*}
0 &< a_{11} \left(- \frac{y^Tz}{a_{11}} \right)^2 - 2 \frac{y^Tz}{a_{11}} y^Tz + y^TCy &\\
&= -\frac{(y^Tz)^2}{a_{11}} + y^TCy &\\
&= y^TC^{(1)}y
\end{align*}
für beliebiges $y \in \mathbb{R}^{n-1}\setminus \{0\}$. \\
Induktiv folgt dann $a_{22}^{(1)} > 0$ $C^{(2)}$ spd, ...\\\\
Zeige nun noch ii):\\
Es gilt 
$$l_{i1} = \frac{a_{i1}}{a_{11}} = \frac{a_{i1}}{r_{11}} = \frac{r_{1i}}{r_{11}}$$
da $r_{1i} = a_{1i} = a_{i1}$ für $i=2,...,n$.\\
Außerdem gilt
$$l_{i2} = \frac{a_{i2}}{a_{22}} = \frac{a_{i2}}{r_{22}} = \frac{r_{2i}}{r_{22}}$$
da $r_{2i} = a_{2i}^{(1)} = a_{i2}^{(1)}$ für $i=3,...,n$.\\
Allgemein gilt also
$$\forall i > j: \medspace l_{ij} = \frac{r_{ji}}{r_{jj}},$$
wobei $r_{ii} = a_{ii}^{(i-1)} > 0$ und $r_{ij} = l_{ij} * r_{jj} = r_{jj} * l_{ij}$, d.h. $R = DL^T$ für 
$$D =
  \begin{bmatrix}
  r_1 && 0 \\
  & \ddots \\
  0 && r_{nn}\\
  \end{bmatrix}
$$
Es wird die i-te Zeile von $L^T$ mit $r_{ii}$ skaliert.
\end{proof}
\end{theorem}

\begin{comment*}
Wegen $R_{ii} > 0$ ist $D = D^{\nicefrac{1}{2}}D^{\nicefrac{1}{2}}$ mit 
$$D^{\nicefrac{1}{2}} =
  \begin{bmatrix}
  \sqrt{r_1} && 0 \\
  & \ddots \\
  0 && \sqrt{r_{nn}} \\
  \end{bmatrix}
$$
Damit erhält man für $\tilde{L} = LD^{\nicefrac{1}{2}}$ (Spaltenskalierung)
$$A = LDL^T = LD^{\nicefrac{1}{2}}D^{\nicefrac{1}{2}}L^T = (LD^{\nicefrac{1}{2}})(LD^{\nicefrac{1}{2}})^T = \tilde{L}\tilde{L}^T$$
Wir bezeichnen die Elemente von $\tilde{L}$ wieder mit $l_{ij}$:
$$\tilde{L} =
  \begin{bmatrix}
  l_{11} && 0 \\
  \vdots& \ddots \\
  l_{1n} &\dots& l_{nn} \\
  \end{bmatrix}
$$
Diese $l_{ij}$'s lassen sich direkt aus der Gleichung $A = \tilde{L}\tilde{L}^T$ berechnen.
\begin{align*}
\begin{bmatrix}
  l_{11} && 0 \\
  \vdots& \ddots \\
  l_{1n} &\dots& l_{nn} \\
\end{bmatrix}
\begin{bmatrix}
  l_{11} &\dots& l_{n1} \\
  & \ddots &\vdots\\
  0 && l_{nn}\\
\end{bmatrix}
=
\begin{bmatrix}
  a_{11}&a_{12} \\
  a_{12}&a_{22}&a_{32} \\
  &&\ddots \\
  &&&a_{nn} \\
\end{bmatrix}
\end{align*}
Nun folgt:
\begin{align*}
&l_{11}^2 = a_{11} > 0 \quad \Rightarrow \quad l_{11} = \sqrt{a_{11}} &\\
&l_{11}l_{i1} = a_{i1} \quad \Rightarrow \quad l_{i1} = \frac{a_{i1}}{l_{11}} &\\
\intertext{allgemein gilt:}
&a_{kk} = l_{k1}^2 + l_{k2}^2 + ... + l_{kk-1}^2 + l_{kk}^2 &\\
&l_{kk} = \left( a_{kk} - \sum_{j=1}^{k-1}l_{kj}^2 \right) ^{\nicefrac{1}{2}}
\intertext{und für $i > k$:}
& a_{ik} = l_{i1}l_{k1} + l_{i2}l_{k2} + ... + l_{ik-1}l_{kk-1} + l_{ik}l_{kk} \\
&l_{ik} = \frac{\left(a_{ik} - \sum_{j=1}^{k-1} l_{ij}l_{kj} \right)}{l_{kk}}
\end{align*}
\underline{Choleski-Verfahren:}
\begin{algorithmic}
\FOR{$k = 1,...,n$}
  \STATE $l_{kk} = \left( a_{kk} - \sum_{j=1}^{k-1}l_{kj}^2 \right) ^{\nicefrac{1}{2}}$
  \FOR{$i=k+1,...,n$}
    \STATE $l_{ik} = \nicefrac{\left(a_{ik} - \sum_{j=1}^{k-1} l_{ij}l_{kj} \right)}{l_{kk}}
$
  \ENDFOR
\ENDFOR
\end{algorithmic}
Nun stellen sich folgende zwei Fragen:
\begin{itemize}
  \item Wie wirken sich Störungen in $A$ und $b$ auf die Lösung von $Ax=b$ aus?
  \item Wie wirken sich Rundungsfehler im Verfahren auf die berechnete Lösung aus?
\end{itemize}
\end{comment*}

\subsection{Matrixnormen}

\begin{definition}
Die Abbildung $\Vert \cdot \Vert: \mathbb{R}^n \rightarrow \mathbb{R}^n, \medspace x \mapsto \Vert x \Vert$ ist eine Norm auf dem $\mathbb{R}$- Vektorraum $\mathbb{R}^n$, falls gilt:
\begin{enumerate}
  \item[i)] $\forall x \in \mathbb{R}^n: \medspace \Vert x \Vert \geq 0$
  \item[ii)] $\Vert x \Vert = 0 \Rightarrow \medspace x = 0 \in \mathbb{R}^n$
  \item[iii)] $\forall \alpha \in \mathbb{R} \forall x \in \mathbb{R}^n: \medspace \Vert \alpha x \Vert = \vert \alpha \vert \Vert x \Vert $
  \item[iv)] $\Vert x + y \Vert \leq \Vert x \Vert + \Vert y \Vert $
\end{enumerate}
\end{definition}

\begin{example} \leavevmode
\begin{enumerate}
  \item[i)] $ \Vert x \Vert_1 = \sum_{j=1}^n \vert x_j \vert$ für $x=(x_1, ...,x_n)^T$
  \item[ii)] $ \Vert x \Vert_2 = \left( \sum_{j=1}^n \vert x_j \vert ^2 \right)^{\nicefrac{1}{2}}$ oder allgemein $\Vert x \Vert_p = \left( \sum_{j=1}^n \vert x_j \vert ^2p\right)^{\nicefrac{1}{p}}$ für $ 1\leq p < \infty$
  \item[iii)] $\Vert x \Vert_{\infty} = \max_{i=1,...,n} \vert x_i\vert$
\end{enumerate}
\end{example}

\begin{definition}
Sei $A\in \mathbb{R}^{m \times n}$, d.h. $A: \mathbb{R}^n \rightarrow \mathbb{R}^m$ linear. Nun heißt
$$\Vert A \Vert_{a \rightarrow b} := \sup_{x \notin \mathcal{O}_n, \thinspace x \in \mathbb{R}^n} \frac{\Vert Ax \Vert_a}{\Vert x\Vert_b}$$
die von den Vektorraumnormen induzierte Norm. Schreibe einfach nur $\Vert \cdot \Vert$.
\end{definition}

\begin{comment}
Sei $A \in \mathbb{R}^{n\times m}$, $\alpha \in \mathbb{R}$ Es gilt für die in (18.3) definierte Matrixnorm
\begin{enumerate}
  \item[i)] $\forall x \in \mathbb{R}^n: \medspace \Vert Ax \Vert \leq \Vert A \Vert \Vert x \Vert$ \\
  $\Vert A \Vert$ ist die kleinste Zahl mit dieser Eigenschaft
  \item[ii)] Es gilt $\Vert A \Vert \geq 0$. Weiter gilt $\Vert A \Vert = 0 \Rightarrow \medspace A = 0$
  \item[iii)] $\Vert \alpha A \Vert = \vert \alpha \vert \Vert A \Vert$
  \item[iv)] $\Vert A + B \Vert \leq \Vert A \Vert + \Vert B \Vert$.\\
	Damit ist $\Vert \cdot \Vert$ tatsächlich eine Norm.
  \item[v)] $\Vert I \Vert = 1$ falls $m=n$, $\Vert \cdot \Vert_{\mathbb{R}^m} = \Vert \cdot \Vert_{\mathbb{R}^n}$
  \item[vi)] $\Vert AB \Vert \leq \Vert A \Vert \thinspace \Vert B \Vert$ (Submultiplikativität)
\end{enumerate}
\end{comment}

\begin{theorem}
Sei $A = (a_{ij})_{i,j=1}^{m,n} \in \mathbb{R}^{m \times n}$. Es gilt für $\Vert A \Vert_p = \sup_{x \neq 0} \frac{\Vert Ax \Vert_p}{\Vert x \Vert_p}$ für $p \geq 1$
\begin{enumerate}
  \item[i)] $\Vert A \Vert_1 = \max_{j=1,...,n} \sum_{i=1}^m \vert a_{ij} \vert$ ist die maximale Spaltenbetragssumme
  \item[ii)] $\Vert A \Vert_2$ ist die Wurzel des größten Eigenwerts von $A^TA$
  \item[iii)] $\Vert A \Vert_{\infty} = \max_{i=1,...,m} \sum_{j=1}^n \vert a_{ij} \vert$ ist die maximale Zeilenbetragssumme
\end{enumerate}
\begin{proof}[Beweis]\leavevmode
\begin{enumerate}
  \item[i) + iii)] Übungsaufgabe
  \item[ii)] $A^TA$ ist symmetrisch und positiv semidefinit. Es gilt nämlich 
  \begin{align*}
  &(A^TA)^T = A^T A^{T^T} = A^TA &\\
  \intertext{und}
  &x^TA^TAx = (Ax)^T Ax = {\Vert Ax \Vert_2}^2 \geq 0 &\\
  \end{align*} 
  Damit ist $A^TA$ orthogonal diagonalisierbar, d.h. es ex. $Q$ mit $Q^TQ = I$ sodass $Q^TA^TAQ = D$ mit 
  $$D = 
  \begin{bmatrix}
  \lambda_1 \\
  & \ddots \\
  && \lambda_m
  \end{bmatrix}$$
  wobei $\lambda_j \geq 0 \medspace(j=1,...,m)$ die Eigenwerte von $A^TA$ sind.\\
  Damit ist
  \begin{align*}
  {\Vert Ax \Vert_2}^2 &= x^TA^TAx \underset{x = Qy}{=} y^TQ^TA^TAQy = \sum_{j=1}^m \lambda_j y_j^2 \\
  & \leq \lambda_{\max} \sum_{j=1}^m y_j^2 = \lambda_{\max} y^Ty = \lambda_{\max} {\Vert y \Vert_2}^2 = \lambda_{\max} {\Vert x \Vert_2}^2
  \end{align*}
  $\Rightarrow \medspace \Vert A \Vert_2 \leq \sqrt{\lambda_{\max}}$ für den größten Eigenwert $\lambda_{\max}$ von $A^TA$.\\
  Sei $\tilde{x} = Q\tilde{y}$ mit $\tilde{y} = (0,...,0,\underset{\text{$j_0$-ter Eintrag}}{1},0,...,0)^T$ mit $\lambda_{j_0} = \lambda_{\max}$. Dann ist ${\Vert A\tilde{x} \Vert_2}^2 = \lambda_{\max} {\Vert \tilde{x} \Vert_2}^2 \Rightarrow \medspace \Vert A \Vert_2 = \sqrt{\lambda_{\max}}$.
\end{enumerate}
\end{proof}
\end{theorem}

\subsection{Kondition eines Problems}

\begin{definition}
Seien $X,Y$ normierte Vektorräume $(X, \Vert \cdot \Vert_X)$, $(Y, \Vert \cdot \Vert_Y)$. Ein Problem bzw. eine Problemstellung ist eine Abbildung $f:X \rightarrow Y$, wobei $X$ die Eingaben und $Y$ die Ausgaben enthält.
\end{definition}

\begin{example}
Sei $X = Y = \mathbb{R}^2$ und $\Vert \cdot \Vert_X = \Vert \cdot \Vert_Y = \Vert \cdot \Vert_2$.
\begin{enumerate}
  \item[i)] $f: (x_1, x_2) \mapsto A \left(\begin{matrix} x_1 \\x_2 \end{matrix} \right)$ für eine $2 \times 2$ Matrix $A$. "Anwendung der linearen Abbildung $A$"
  \item[ii)] $f: (p,q) \mapsto \text{Wurzeln von $z^2 + pz + q = 0$}$. "Berechnung der Wurzeln eines normierten quadratischen Polynoms"
\end{enumerate}
\end{example}

\begin{definition}[Absolute Kondition]
Seien $X, Y$ normierte Vektorräume $f: X \rightarrow Y$ ein Problem. Die \textbf{absolute Kondition} von $f$ in $x \in X$ ist 
$$\kappa_{\text{abs}}(f,x) := \lim_{\delta \rightarrow 0} \sup_{\Vert z \Vert_X \leq \delta} \frac{\Vert f(x+z) - f(x) \Vert_Y}{\Vert z \Vert_X} $$
%    \begin{figure}[!htb]
%    \centering
%    \includegraphics[width=16cm]{Bild5.png}
%    \end{figure}\\
\end{definition}

\begin{lemma}
Sei $f: (\mathbb{R}, \Vert \cdot \Vert) \rightarrow (\mathbb{R}, \Vert \cdot \Vert)$ differenzierbar, so gilt
$$\kappa_{\text{abs}}(f,x) = \vert f'(x) \vert$$
\begin{proof}[Beweis]
Übungsaufgabe.
\end{proof}
\end{lemma}

\begin{definition}[Relative Kondition]
Seien $X, Y$ normierte Räume, $f: X \rightarrow Y$ ein Problem. Die \textbf{relative Kondition} von $f$ in $x \in X$ ist
$$\kappa_{\text{rel}}(f,x) := \lim_{\delta \rightarrow 0} \sup_{\Vert z \Vert_X \leq \delta} \frac{\frac{\Vert f(x+z) - f(x) \Vert_Y}{\Vert f(x) \Vert_Y}}{\frac{\Vert z \Vert_X}{\Vert x \Vert_X}} $$
d.h. $\kappa_{\text{rel}}(f,x)$ ist die kleinste Zahl sodass 
$$\underbrace{\frac{\Vert f(x) - f(x+z) \Vert_Y}{\Vert f(x) \Vert_Y}}_{\underset{\text{statt f(x) das Problem f(x+z) gelöst hat}}{\text{relativer Fehler in der Ausgabe, wenn man}}} \leq \kappa_{\text{rel}}(f,x) \underbrace{\frac{\Vert z \Vert_X}{\Vert x \Vert_X}}_{\text{relativer Fehler in der Eingabe}}$$
\end{definition}

\begin{example}
Kondition der Addition:
\begin{align*}
&f: (\mathbb{R}^2, \Vert \cdot \Vert_1) \rightarrow (\mathbb{R}, \vert \cdot \vert), \medspace (a, b) \mapsto a+b &\\
&\kappa_{\text{abs}}(f,(a,b)) = \lim_{\delta \rightarrow 0} \sup_{\vert \alpha \vert + \vert \beta \vert \leq \delta, \medspace z = (\alpha, \beta)} \frac{\vert a + \alpha + b + \beta - a - b \vert}{\vert \alpha \vert + \vert \beta \vert} = 1 &\\
&\kappa_{\text{rel}}(f,(a,b)) = ... = \frac{^\vert a \vert + \vert b \vert}{\vert a+b \vert} &\\
\end{align*}
d.h. für die Addition zweier Zahlen mit gleichem Vorzeichen ist $\kappa_{\text{rel}} = 1$. Für Subtraktion zweier annähernd gleich großer Zahlen ist $\kappa_{\text{rel}}$ groß.
\end{example}

\begin{definition}
Ein Problem heißt \textbf{gut konditioniert}, falls $\kappa_{\text{rel}}$ klein ist ($< 10^3$) und \textbf{schlecht konditioniert}, falls $\kappa_{\text{rel}}$ groß ist ($>10^8$).
\end{definition}

\subsection{Konditionszahl einer Matrix}
Gegeben sei $Ax = b$. Welchen Einfluss haben Fehler in $A$ und in $b$ auf die Lösung $x$?\\
In Form von $\S 19$: $f: (A, b) \mapsto x$. Statt $a_{ij}$ stehen nun $\tilde{a}_{ij}= a_{ij}(1+ \varepsilon_{ij})$ und statt $b_i$ nun $b_i(1+ \varepsilon_{i})$ zur Verfügung. Also $\tilde{A} \tilde{x} = \tilde{b}$. 

\begin{theorem}
Sei $A$ invertierbar, $A \in \mathbb{R}^{n \times n}$, $Ax = b$, $\tilde{A} \tilde{x} = \tilde{b}$, $x \neq 0$. \\
Falls 
$$ \frac{\Vert A - \tilde{A} \Vert}{\Vert A \Vert} \leq \varepsilon_A, \quad \frac{\Vert b - \tilde{b} \Vert}{\Vert b \Vert} \leq \varepsilon_b$$
so gilt für die Lösung des LGS:
$$\frac{\Vert \tilde{x} - x \Vert }{\Vert x \Vert} \leq \frac{\text{cond($A$)}}{1- \varepsilon_A \text{cond($A$)}}(\varepsilon_A + \varepsilon_b)$$
falls $\varepsilon_A *\text{cond($A$)} < 1$. Hierbei ist $\text{cond($A$)} = \Vert A \Vert \Vert A^-1 \Vert$ die Konditionszahl von $A$ und die Matrixnorm wird von der Vektorraumnorm induziert (vgl. 18.3)
\begin{proof}[Beweis]
\begin{align*}
b - \tilde{b} &= Ax - \tilde{A}\tilde{x} &\\
&= Ax - A\tilde{x} + A\tilde{x} - \tilde{A}\tilde{x} &\\
&= A(x-\tilde{x}) + (A- \tilde{A})\tilde{x} &\\
\Rightarrow x- \tilde{x} &= A^{-1} (b -\tilde{b} - (A - \tilde{A})\tilde{x}) &\\
\Vert x - \tilde{x} \Vert & \leq \Vert A^{-1} \Vert ( \Vert b \Vert \varepsilon_b + \varepsilon_A \Vert A \Vert \Vert \tilde{x} \Vert ) &\\
& \underset{b=Ax}{\leq} \text{cond($A$)} ( \Vert x \Vert \varepsilon_b + \varepsilon_A (\Vert \tilde{x} - x \Vert + \Vert x \Vert) &\\
\Rightarrow (1- \text{cond($A$)} \varepsilon_A) \Vert - \tilde{x} \Vert & \leq \text{cond($A$)} \Vert x \Vert (\varepsilon_b + \varepsilon_A)
\end{align*}
\end{proof}
\end{theorem}

\begin{comment}
Die Abschätzung aus (20.1) ist scharf, d.h. es gibt $\tilde{A}$ und $\tilde{b}$, sodass Gleichheit gilt. Aber sie ist oft zu pessimistisch für Rundungsfehlerabschätzungen.
\begin{description}
  \item[Beispiel:] Betrachte folgendes Gleichungssystem: 
    \begin{align*}
    \underbrace{\left( \begin{matrix}
    1 & 1 \\
    0 & 10^{-8} \\
    \end{matrix} \right)}_A
    \left( \begin{matrix}x_1\\x_2\end{matrix} \right)
    =
    \left( \begin{matrix}b_1\\b_2\end{matrix} \right)
    \end{align*}
    Sei des Weiteren $\vert \varepsilon_j \vert < eps$. \\
    Es gilt $\text{cond}_{\infty}(A) = \Vert A \Vert_{\infty} \Vert A^{-1} \Vert_{\infty} = 2 * 10^8$. Das gestörte System ist nun:
    \begin{align*}
    (1+ \varepsilon_1) \tilde{x}_1 + (1 + \varepsilon_2) \tilde{x}_2 &= \overbrace{b_1 ( 1+ \varepsilon_3)}^{= \tilde{b}_1} &\\
    (1+\varepsilon_4) 10^{-8} \tilde{x}_2 &= b_2(1+\varepsilon_5)
    \end{align*}
    
    \begin{align*}
    \intertext{Mit $\frac{1}{1+\varepsilon} = 1 - \varepsilon + \varepsilon^2 - \varepsilon^3 + \varepsilon^4 - ...$ folgt}
    &\Rightarrow \tilde{x}_2 = 10^8 b_2 \frac{1+\varepsilon_5}{1+\varepsilon_4} = 10^8 b_2(1+\varepsilon_5 - \varepsilon_4) + \mathcal{O}(eps^2) &\\
    &\Rightarrow \frac{\vert x_2 - \tilde{x}_2 \vert}{\vert x_2 \vert} \leq 2 eps 
    \end{align*}
    
    \begin{alignat*}{2}
    &\tilde{x}_1 &&= [b_1(1+\varepsilon_3) - x_2(1+\varepsilon_5 - \varepsilon_4 + \varepsilon_3)] (1-\varepsilon_1) + \mathcal{O}(eps^2) \\
    & &&= [x_1 + \underbrace{b_1}_{=x_1 + x_2} \varepsilon_3 - x_2(\varepsilon_5 - \varepsilon_4 + \varepsilon_2)] (1-\varepsilon_1) + \mathcal{O}(eps^2)
    \end{alignat*}
    
    \begin{align*}
    &\tilde{x}_1-x_1 = x_1(-\varepsilon_1 + \varepsilon_3) - x_2(-\varepsilon_3 + \varepsilon_5 - \varepsilon_4 + \varepsilon_2) + \mathcal{O}(eps^2)&\\
    &\frac{\vert \tilde{x}_1 - x_1 \vert}{\vert x_1 \vert} \leq ( 2 + 4 \frac{\vert x_2 \vert}{\vert x_1 \vert} ) eps
    \end{align*}
    Dieser Wert kann sehr groß werden für $\frac{\vert x_2 \vert}{\vert x_1 \vert} \rightarrow \infty$, aber $\frac{\vert \tilde{x}_1 - x_1 \vert}{\Vert x_1 \Vert_{\infty}} \leq 6eps$.
\end{description} 
\end{comment}

\begin{lemma}
Sei $A \in \mathbb{R}^{n \times n}$ invertierbar. Dann gilt:
\renewcommand{\labelenumi}{\roman{enumi})}
\begin{enumerate}
  \item $\text{cond}(A) \geq 1$
  \item $\forall \alpha \in \mathbb{R}\setminus \{0\}: \medspace \text{cond}(\alpha A) = \text{cond}(A)$
  \item $ \text{cond}(A) = \frac{\max_{\Vert y \Vert = 1} \Vert Ay \Vert}{\min_{\Vert x \Vert = 1} \Vert A^{-1}x \Vert}$
\end{enumerate}
\begin{proof}[Beweis]
Übungsaufgabe.
\end{proof}
\end{lemma}

\begin{example}\leavevmode
\renewcommand{\labelenumi}{\theenumi) }
\begin{enumerate}
  \item Matrizen mit kleiner Konditionszahl:
    \begin{itemize}
      \item $I$ mit $\text{cond}(I) = 1$
      \item orthogonale Matrizen $Q$ ($Q^TQ = I$)
        \begin{align*}
        &{\Vert Qx \Vert_2}^2 = x^TQ^T Q x = x^Tx = {\Vert x \Vert_2}^2 \Rightarrow \Vert Q \Vert_2 = 1 &\\
        & Q^{-1} = Q^T \Rightarrow \Vert Q^{-1} \Vert_2 = 1 \Rightarrow \text{cond}_2(Q) = 1
        \end{align*}
      \item Splineinterpolationsmatrix ($h_i = h$)
        \begin{align*}
        &A = \frac{1}{h} \left[ \begin{matrix} 4 & 1 & & 0 \\ 1 & \ddots & \ddots \\  & \ddots & & 1 \\ 0 & & 1 & 4\end{matrix} \right], \quad \Vert A \Vert_{\infty} = 6 &\\
        &A = 4(I+N) \quad \text{mit } N = \left[ \begin{matrix} 0 & \nicefrac{1}{4} & & 0 \\ \nicefrac{1}{4} & \ddots & \ddots \\  & \ddots & & \nicefrac{1}{4} \\ 0 & & \nicefrac{1}{4} & 0 \end{matrix} \right] &\\
        &A^{-1} = \nicefrac{1}{4}(I+N)^{-1} = \nicefrac{1}{4} (I-N+N^2-N^3 + ...), \quad \Vert N \Vert_{\infty} = \nicefrac{1}{2} &\\
        &\Vert A ^{-1} \Vert \leq \nicefrac{1}{4}(\Vert I \Vert + \Vert N \Vert + \Vert N \Vert ^2 + ...) = \nicefrac{1}{2}
        \end{align*}
    \end{itemize}
  
  \item Matrizen mit großer Konditionszahl: 
    \begin{itemize}
      \item Hilbertmatrix $H = \left(\frac{1}{i+j-1}\right)_{i,j=1}^n$
        $$ H = \left[ \begin{matrix}1 & \nicefrac{1}{2} & \nicefrac{1}{3} & \nicefrac{1}{4} & \dots \\ \nicefrac{1}{2} & \nicefrac{1}{3} & \nicefrac{1}{4} \\ \nicefrac{1}{3} & \nicefrac{1}{4} \\ \nicefrac{1}{4} \\ \vdots \end{matrix} \right]$$
        Für $n \in \{1,..., 10\}$ ergibt sich folgende Tabelle:\\
        \begin{tabular}{c|c}
        $n$ & $\text{cond}_2(A)$ \\
        \hline
        1 & 1 \\
        2 & 27 \\
        3 & 740 \\
        4 & 2300 \\
        \vdots & \vdots \\
        10 & $35 * 10^{13}$
        \end{tabular}
    \end{itemize}
\end{enumerate}
\end{example}

\subsection{Stabilität von Verfahren}
\begin{definition}
Ein Verfahren zur Auswertung eines Problems $f$ ist die Hintereinanderausführung von elementaren Operationen $\tilde{f}_k$ \\
$\tilde{f} = \tilde{f}_n \circ \tilde{f}_{n-1} \circ ... \circ \tilde{f}_1, \quad \tilde{f}_k \in \{ +, -, *, /, fl, \sqrt{\cdot}, ...\} $
\end{definition}

\begin{definition}
Ein Verfahren zur Auswertung des Problems $f$ ist stabil im Sinne der Vorwärtsanalysis, falls 
$$\tilde{f}(x) - f(x) \Vert < C * eps *  \Vert f(x) \Vert$$
für eine nicht zu große Konstante $C$.
\end{definition}

\begin{example}
Berechnung von $\frac{1}{x(x-1)}$ für $x = 10^4$.
\renewcommand{\labelenumi}{\theenumi) }
\begin{enumerate}
  \item
    \begin{tabular}{ccccccc}
    &\rotatebox[origin=r]{30}{$\rightarrow$} & $fl(x)$ & \rotatebox[origin=l]{-30}{$\rightarrow$} \\
    x &&&&$fl(x(x-1))$& $\rightarrow$ & $fl(\frac{1}{x(x-1)})$ \\
    &\rotatebox[origin=r]{-30}{$\rightarrow$} & $fl(x-1)$ & \rotatebox[origin=l]{30}{$\rightarrow$} 
    \end{tabular}
    
  \item
    \begin{tabular}{cccccccc}
    &\rotatebox[origin=r]{30}{$\rightarrow$} & $fl(x)$ & $\rightarrow$ & $fl(\frac{1}{x})$ & \rotatebox[origin=l]{-30}{$\rightarrow$} \\
    x &&&&&& $fl(-\frac{1}{x} + \frac{1}{x-1})$ \\
    &\rotatebox[origin=r]{-30}{$\rightarrow$} & $fl(x-1)$ & $\rightarrow$ & $fl(\frac{1}{x-1})$ & \rotatebox[origin=l]{30}{$\rightarrow$} 
    \end{tabular}
\end{enumerate}
Verfahren 2) ist nicht stabil, da $\frac{1}{x} \approx \frac{1}{x-1}$, falls $x=10^4$ gilt und die Subtraktion im letzten Schritt schlecht konditioniert ist. 
\end{example}

\begin{definition}
Ein Verfahren $\tilde{f}$ zur Auswertung eines Problems $f$ ist stabil im Sinne der Rückwärtsanalysis, falls für jedes $x \in X$ ein $\tilde{x} \in X$ existiert, sodass
$$ \tilde{f}(x) = f(\tilde{x}) \quad \text{mit } \frac{\Vert \tilde{x} - x \Vert }{\Vert x \Vert} \leq C*eps$$
für eine nicht zu große Konstante $C$. Die berechnete Lösung $\tilde{f}(x)$ kann als exakte Lösung eines benachbarten Problems $f(\tilde{x})$ aufgefasst werden.
\end{definition}

\begin{example}
Zur Berechnung von $x_1x_2 + x_3x_4$ verwendet man:\\
\begin{tabular}{ccccc}
&\rotatebox[origin=c]{30}{$\underrightarrow{\cdot}$} & $x_1x_2$ & \rotatebox[origin=l]{-30}{$\rightarrow$} \\
$(x_1, x_2, x_3, x_4)$ & & & + & $x_1x_2 + x_3x_4$ \\
&\rotatebox[origin=c]{-30}{$\overrightarrow{\cdot}$} & $x_1x_2$ & \rotatebox[origin=l]{30}{$\rightarrow$} \\
\end{tabular}\\
und erhält unter Berücksichtigung von Rundungsfehlern\\
$$\left[(x_1 ( 1 + \varepsilon_1) x_2(1+\varepsilon_2))(1+\eta_1) + (x_3(1+\varepsilon_3)x_4(1+\varepsilon_4))(1+\eta_2)\right](1+\eta_3) \quad \text{für $\vert \varepsilon_j \vert, \vert \eta_j \vert \leq eps$}$$
Das ist das exakte Ergebnis für 
\begin{flalign*}
&\tilde{x}_1 = x_1(1+ \varepsilon_1)(1+\eta_1)(1+\eta_3)&\\
&\tilde{x}_2 = x_2(1+ \varepsilon_2)(1+\eta_1)(1+\eta_3)&\\
&\tilde{x}_3 = x_3(1+ \varepsilon_3)(1+\eta_2)(1+\eta_3)&\\
&\tilde{x}_4 = x_4(1+ \varepsilon_4)(1+\eta_2)(1+\eta_3)
\end{flalign*}
Die Konstante $C$ in (21.4) ist etwa 3, wenn man Produkte von $\varepsilon_j$ und $\eta_j$ vernachlässigt. Das Verfahren ist also rückwärtsstabil, auch wenn evtl. das Problem schlecht konditioniert ist.
\end{example}

\begin{theorem}[Stabilität der Gaußelimination (LR-Zerlegung)]
Sei $A \in \mathbb{R}^{n \times n}$ invertierbar und $\hat{L}\hat{R}$ das rundungsfehlerbehaftete Ergebnis der Gaußelimination mit Pivotisierung, sodass $\vert \hat{l}_{ij} \vert \leq 1$ für alle $i,j \in \{1,..., n\}$. \\
Dann gilt für $\hat{A} = (\hat{a}_{ij})_{i,j=1}^n = \hat{L}\hat{R}$:
$$\vert a_{ij} - \hat{a}_{ij} \vert \leq 2 \max_{i,j,k} \vert a_{ij}^{(k)} \vert * \min\{i-1, j\} * eps$$
für Maschinengenauigkeit $eps$.
\begin{proof}[Beweis]
Im k-ten Schritt berechnet man ausgehend von $\hat{a}_{ij}^{(k-1)}$
\begin{align*}
\hat{a}_{ij}^{(k)} &= \left( \hat{a}_{ij}^{(k-1)} - \hat{l}_{ik} \hat{a}_{kj}^{(k-1)} ( 1+ \varepsilon_{ijk}) \right) ( 1+ \eta_{ijk}) &\\
&= \hat{a}_{ij}^{(k-1)} - \hat{l}_{ik} \hat{a}_{kj}^{(k-1)} + \mu_{ijk} \quad (*)
\end{align*}
mit $\vert \varepsilon_{ijk} \vert, \vert \eta_{ijk} \vert \leq eps$ und $\mu_{ijk} \leq \vert \hat{a}_{ij}^{(k-1)} \vert \vert \eta_{ijk} \vert + \vert \hat{l}_{ik} \vert \vert \hat{a}_{kj}^{(k-1)} \vert \vert \varepsilon_{ijk} \vert + \mathcal{O}(eps^2). \quad (**)$\\
Nach Definition von $\hat{A}$ ist 
$$\hat{a}_{ij} = \sum_{k=1}^{\min\{i,j\}} \hat{l}_{ik} \hat{r}_{kj} = \sum_{k=1}^{\min\{i,j\}} \hat{l}_{ik} \hat{a}_{kj}^{(k-1)}$$ 
Verwendet man für $i > j$ (*), so erhält man
$$\hat{a}_{ij} = \sum_{k=1}^j \left( \hat{a}_{ij}^{(k-1)} - \hat{a}_{ij}^{(k)} + \mu_{ijk} \right) = a_{ij} + \sum_{k=1}^j \mu_{ijk}, \quad \text{da } a_{ij}^{(j)} = 0$$
Für $i \leq j $ erhält man
$$\hat{a}_{ij} = \sum_{k=1}^{i-1} \left( \hat{a}_{ij}^{(k-1)} - \hat{a}_{ij}^{(k)} + \mu_{ijk} \right) + \hat{l}_{ii} a_{ij}^{(i-1)}= a_{ij} + \sum_{k=1}^{i-1} \mu_{ijk}, \quad \text{da } \hat{l}_{ii} = 1$$
Zusammen mit (**) folgt die Behauptung.
\end{proof}
\end{theorem}
%entklammere Graphiken mit \includegraphics wieder 
\end{document}
